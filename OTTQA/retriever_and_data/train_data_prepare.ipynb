{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f45dbd-59a2-4638-a881-215386aae9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Set paths\n",
    "BASE_PATH = \"Data/HybridQA\"\n",
    "TRAIN_FILE = os.path.join(BASE_PATH, \"train.p.json\")\n",
    "TABLES_PATH = os.path.join(BASE_PATH, \"WikiTables-WithLinks\", \"tables_tok\")\n",
    "\n",
    "# Function: Load specified number of training data\n",
    "def load_train_data(file_path, num_samples=None):\n",
    "    with open(file_path, 'r') as f:\n",
    "        train_data = json.load(f)\n",
    "    \n",
    "    if num_samples is not None and num_samples < len(train_data):\n",
    "        train_data = random.sample(train_data, num_samples)\n",
    "    \n",
    "    print(f\"Loaded {len(train_data)} training examples.\")\n",
    "    return train_data\n",
    "\n",
    "# Function: Load and process tables\n",
    "def load_table(table_id):\n",
    "    table_file = os.path.join(TABLES_PATH, f\"{table_id}.json\")\n",
    "    with open(table_file, 'r') as f:\n",
    "        table_data = json.load(f)\n",
    "    \n",
    "    headers = [h[0] for h in table_data['header']]\n",
    "    rows = []\n",
    "    for row in table_data['data']:\n",
    "        row_dict = {}\n",
    "        for i, cell in enumerate(row):\n",
    "            row_dict[headers[i]] = cell[0]\n",
    "        rows.append(row_dict)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Prepare training data\n",
    "def prepare_training_data(train_data, num_negative_samples=3):\n",
    "    prepared_data = []\n",
    "    for item in tqdm(train_data, desc=\"Preparing training data\"):\n",
    "        question = item['question']\n",
    "        table_id = item['table_id']\n",
    "        labels = item['labels']\n",
    "        \n",
    "        # Load table\n",
    "        table = load_table(table_id)\n",
    "        \n",
    "        # Find positive samples (rows with label 1)\n",
    "        positive_indices = [i for i, label in enumerate(labels) if label == 1]\n",
    "        \n",
    "        for pos_idx in positive_indices:\n",
    "            positive_row = table.iloc[pos_idx].to_dict()\n",
    "            \n",
    "            # Select negative samples\n",
    "            negative_indices = [i for i, label in enumerate(labels) if label == 0]\n",
    "            negative_samples = random.sample(negative_indices, min(num_negative_samples, len(negative_indices)))\n",
    "            \n",
    "            for neg_idx in negative_samples:\n",
    "                negative_row = table.iloc[neg_idx].to_dict()\n",
    "                \n",
    "                prepared_data.append({\n",
    "                    'question': question,\n",
    "                    'positive_row': positive_row,\n",
    "                    'negative_row': negative_row,\n",
    "                    'table_id': table_id\n",
    "                })\n",
    "    \n",
    "    return prepared_data\n",
    "\n",
    "# User input for the amount of data to process\n",
    "num_samples = int(input(\"Enter the number of training samples to process (or 0 for all): \"))\n",
    "if num_samples <= 0:\n",
    "    num_samples = None\n",
    "\n",
    "# Load training data\n",
    "train_data = load_train_data(TRAIN_FILE, num_samples)\n",
    "\n",
    "# Prepare training data\n",
    "training_data = prepare_training_data(train_data)\n",
    "print(f\"Prepared {len(training_data)} training samples.\")\n",
    "\n",
    "# Show a sample\n",
    "sample = random.choice(training_data)\n",
    "print(\"Sample training data:\")\n",
    "print(f\"Question: {sample['question']}\")\n",
    "print(f\"Positive row: {sample['positive_row']}\")\n",
    "print(f\"Negative row: {sample['negative_row']}\")\n",
    "print(f\"Table ID: {sample['table_id']}\")\n",
    "\n",
    "# Save prepared training data\n",
    "output_file = os.path.join(BASE_PATH, f\"prepared_dpr_training_data_{len(train_data)}_samples.json\")\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(training_data, f)\n",
    "print(f\"Saved prepared training data to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bacdce8-e26b-4438-a3f6-0d71ddb9390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"Data/HybridQA\"\n",
    "TEST_FILE = os.path.join(BASE_PATH, \"dev.p.json\")\n",
    "TABLES_PATH = os.path.join(BASE_PATH, \"WikiTables-WithLinks\", \"tables_tok\")\n",
    "\n",
    "def load_test_data(file_path, num_samples=None):\n",
    "    with open(file_path, 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "    \n",
    "    if num_samples is not None and num_samples < len(test_data):\n",
    "        test_data = random.sample(test_data, num_samples)\n",
    "    \n",
    "    print(f\"Loaded {len(test_data)} test examples.\")\n",
    "    return test_data\n",
    "\n",
    "def load_table(table_id):\n",
    "    table_file = os.path.join(TABLES_PATH, f\"{table_id}.json\")\n",
    "    with open(table_file, 'r') as f:\n",
    "        table_data = json.load(f)\n",
    "    \n",
    "    headers = [h[0] for h in table_data['header']]\n",
    "    rows = []\n",
    "    for row in table_data['data']:\n",
    "        row_dict = {}\n",
    "        for i, cell in enumerate(row):\n",
    "            row_dict[headers[i]] = cell[0]\n",
    "        rows.append(row_dict)\n",
    "    \n",
    "    return rows\n",
    "\n",
    "def prepare_test_data(test_data):\n",
    "    prepared_data = []\n",
    "    for item in tqdm(test_data, desc=\"Preparing test data\"):\n",
    "        question = item['question']\n",
    "        table_id = item['table_id']\n",
    "        labels = item['labels']\n",
    "        \n",
    "        table_rows = load_table(table_id)\n",
    "        \n",
    "        assert len(labels) == len(table_rows), f\"Mismatch in labels and rows for table {table_id}\"\n",
    "        \n",
    "        correct_row_index = labels.index(1) if 1 in labels else -1\n",
    "        \n",
    "        prepared_data.append({\n",
    "            'question': question,\n",
    "            'table_id': table_id,\n",
    "            'rows': table_rows,\n",
    "            'correct_row_index': correct_row_index\n",
    "        })\n",
    "    \n",
    "    return prepared_data\n",
    "\n",
    "num_samples = int(input(\"Enter the number of test samples to process (or 0 for all): \"))\n",
    "if num_samples <= 0:\n",
    "    num_samples = None\n",
    "\n",
    "test_data = load_test_data(TEST_FILE, num_samples)\n",
    "\n",
    "prepared_test_data = prepare_test_data(test_data)\n",
    "\n",
    "print(f\"Prepared {len(prepared_test_data)} test samples.\")\n",
    "\n",
    "sample = random.choice(prepared_test_data)\n",
    "print(\"\\nSample test data:\")\n",
    "print(f\"Question: {sample['question']}\")\n",
    "print(f\"Table ID: {sample['table_id']}\")\n",
    "print(f\"Number of rows: {len(sample['rows'])}\")\n",
    "print(f\"Correct row index: {sample['correct_row_index']}\")\n",
    "if sample['correct_row_index'] != -1:\n",
    "    print(f\"Correct row: {sample['rows'][sample['correct_row_index']]}\")\n",
    "\n",
    "output_file = os.path.join(BASE_PATH, f\"prepared_dpr_test_data_{len(test_data)}_samples.json\")\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(prepared_test_data, f)\n",
    "\n",
    "print(f\"\\nSaved prepared test data to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "553229a7-f8d9-47c4-b8ee-18b36931bfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading prepared training data from Data/HybridQA/prepared_dpr_training_data_1_samples.json\n",
      "Loaded 21 prepared training samples.\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|███████████████████████████████████| 1/1 [00:11<00:00, 11.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded data saved.\n",
      "Question embeddings shape: (21, 768)\n",
      "Positive embeddings shape: (21, 768)\n",
      "Negative embeddings shape: (21, 768)\n",
      "\n",
      "Sample embeddings:\n",
      "Question: [-0.5504775   0.31225264 -0.07330009 -0.06485221 -0.30499658]...\n",
      "Positive: [-0.6236328  -0.2962152  -0.18471776 -0.6544239  -0.531056  ]...\n",
      "Negative: [-0.5705106  -0.17978194 -0.13786103 -0.3468474  -0.24918976]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 设置路径\n",
    "BASE_PATH = \"Data/HybridQA\"\n",
    "PREPARED_DATA_FILE = os.path.join(BASE_PATH, \"prepared_dpr_training_data_1_samples.json\")\n",
    "\n",
    "# DPR模型定义\n",
    "class DPRModel(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased'):\n",
    "        super(DPRModel, self).__init__()\n",
    "        self.question_encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.passage_encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def encode_question(self, input_ids, attention_mask):\n",
    "        return self.question_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "\n",
    "    def encode_passage(self, input_ids, attention_mask):\n",
    "        return self.passage_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "\n",
    "# 数据集类\n",
    "class DPRDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        question = item['question']\n",
    "        positive_row = ' '.join(str(v) for v in item['positive_row'].values())\n",
    "        negative_row = ' '.join(str(v) for v in item['negative_row'].values())\n",
    "\n",
    "        question_encoding = self.tokenizer(question, truncation=True, padding='max_length', \n",
    "                                           max_length=self.max_length, return_tensors='pt')\n",
    "        positive_encoding = self.tokenizer(positive_row, truncation=True, padding='max_length', \n",
    "                                           max_length=self.max_length, return_tensors='pt')\n",
    "        negative_encoding = self.tokenizer(negative_row, truncation=True, padding='max_length', \n",
    "                                           max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'question': {k: v.squeeze(0) for k, v in question_encoding.items()},\n",
    "            'positive': {k: v.squeeze(0) for k, v in positive_encoding.items()},\n",
    "            'negative': {k: v.squeeze(0) for k, v in negative_encoding.items()}\n",
    "        }\n",
    "\n",
    "# 编码函数\n",
    "def encode_data(model, dataset, device):\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    question_embeddings = []\n",
    "    positive_embeddings = []\n",
    "    negative_embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Encoding\"):\n",
    "            q_emb = model.encode_question(\n",
    "                batch['question']['input_ids'].to(device),\n",
    "                batch['question']['attention_mask'].to(device)\n",
    "            )\n",
    "            p_emb = model.encode_passage(\n",
    "                batch['positive']['input_ids'].to(device),\n",
    "                batch['positive']['attention_mask'].to(device)\n",
    "            )\n",
    "            n_emb = model.encode_passage(\n",
    "                batch['negative']['input_ids'].to(device),\n",
    "                batch['negative']['attention_mask'].to(device)\n",
    "            )\n",
    "            \n",
    "            question_embeddings.extend(q_emb.cpu().numpy())\n",
    "            positive_embeddings.extend(p_emb.cpu().numpy())\n",
    "            negative_embeddings.extend(n_emb.cpu().numpy())\n",
    "    \n",
    "    return np.array(question_embeddings), np.array(positive_embeddings), np.array(negative_embeddings)\n",
    "\n",
    "# 加载保存的训练数据\n",
    "print(f\"Loading prepared training data from {PREPARED_DATA_FILE}\")\n",
    "with open(PREPARED_DATA_FILE, \"r\") as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(training_data)} prepared training samples.\")\n",
    "\n",
    "# 初始化设备、tokenizer和模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = DPRModel('bert-base-uncased').to(device)\n",
    "\n",
    "# 创建数据集\n",
    "dataset = DPRDataset(training_data, tokenizer)\n",
    "\n",
    "# 编码数据\n",
    "question_embeddings, positive_embeddings, negative_embeddings = encode_data(model, dataset, device)\n",
    "\n",
    "# 保存编码后的向量\n",
    "np.save(os.path.join(BASE_PATH, \"question_embeddings.npy\"), question_embeddings)\n",
    "np.save(os.path.join(BASE_PATH, \"positive_embeddings.npy\"), positive_embeddings)\n",
    "np.save(os.path.join(BASE_PATH, \"negative_embeddings.npy\"), negative_embeddings)\n",
    "\n",
    "print(\"Encoded data saved.\")\n",
    "\n",
    "# 展示编码结果的形状\n",
    "print(f\"Question embeddings shape: {question_embeddings.shape}\")\n",
    "print(f\"Positive embeddings shape: {positive_embeddings.shape}\")\n",
    "print(f\"Negative embeddings shape: {negative_embeddings.shape}\")\n",
    "\n",
    "# 展示一个样本的编码结果\n",
    "print(\"\\nSample embeddings:\")\n",
    "print(f\"Question: {question_embeddings[0][:5]}...\")  # 只显示前5个元素\n",
    "print(f\"Positive: {positive_embeddings[0][:5]}...\")\n",
    "print(f\"Negative: {negative_embeddings[0][:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd22cd64-1e6b-4bc0-990e-77972e6f546e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading prepared training data from Data/HybridQA/prepared_dpr_training_data_1_samples.json\n",
      "Loaded 21 prepared training samples.\n",
      "Loading test data from Data/HybridQA/prepared_dpr_test_data_1_samples.json\n",
      "Loaded 1 test samples.\n",
      "Starting model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|███████████████████████████| 2/2 [01:41<00:00, 50.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Average Loss: 0.0814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|███████████████████████████| 2/2 [01:44<00:00, 52.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Average Loss: 0.0850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|███████████████████████████| 2/2 [01:47<00:00, 53.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Average Loss: 0.0622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|███████████████████████████| 2/2 [01:44<00:00, 52.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Average Loss: 0.0370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|███████████████████████████| 2/2 [01:46<00:00, 53.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Average Loss: 0.0070\n",
      "Encoding training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|███████████████████████████████████| 1/1 [00:13<00:00, 13.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded data saved.\n",
      "Question embeddings shape: (21, 768)\n",
      "Positive embeddings shape: (21, 768)\n",
      "Negative embeddings shape: (21, 768)\n",
      "\n",
      "Sample embeddings:\n",
      "Question: [-1.1217889   0.34976014 -0.4115776  -0.44641808 -0.4475675 ]...\n",
      "Positive: [-0.4455727  -0.06987131 -0.17461737 -0.593914   -0.49142316]...\n",
      "Negative: [-0.21959794 -0.02402857 -0.05738551 -0.30179015 -0.08461066]...\n",
      "\n",
      "Test Results:\n",
      "Table ID: List_of_libraries_in_Barcelona_0\n",
      "Predicted Row: 5\n",
      "Correct Row: 1\n",
      "Retrieved Row Content: {'Name': 'Canyelles', 'Locality': 'Canyelles', 'District': 'Nou Barris', 'Opened': '1994', 'Named after': \"The neighbourhood 's name\", 'Transport links': 'Canyelles'}\n",
      "---\n",
      "Test results saved to predict_result.json\n"
     ]
    }
   ],
   "source": [
    "# 单元格 1: 导入和设置\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 设置路径\n",
    "BASE_PATH = \"Data/HybridQA\"\n",
    "PREPARED_DATA_FILE = os.path.join(BASE_PATH, \"prepared_dpr_training_data_1_samples.json\")\n",
    "TEST_DATA_FILE = os.path.join(BASE_PATH, \"prepared_dpr_test_data_1_samples.json\")\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 单元格 2: 模型定义\n",
    "class DPRModel(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased'):\n",
    "        super(DPRModel, self).__init__()\n",
    "        self.question_encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.passage_encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def encode_question(self, input_ids, attention_mask):\n",
    "        return self.question_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "\n",
    "    def encode_passage(self, input_ids, attention_mask):\n",
    "        return self.passage_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "\n",
    "# 单元格 3: 数据集类定义\n",
    "class DPRDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        question = item['question']\n",
    "        positive_row = ' '.join(str(v) for v in item['positive_row'].values())\n",
    "        negative_row = ' '.join(str(v) for v in item['negative_row'].values())\n",
    "\n",
    "        question_encoding = self.tokenizer(question, truncation=True, padding='max_length', \n",
    "                                           max_length=self.max_length, return_tensors='pt')\n",
    "        positive_encoding = self.tokenizer(positive_row, truncation=True, padding='max_length', \n",
    "                                           max_length=self.max_length, return_tensors='pt')\n",
    "        negative_encoding = self.tokenizer(negative_row, truncation=True, padding='max_length', \n",
    "                                           max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'question': {k: v.squeeze(0) for k, v in question_encoding.items()},\n",
    "            'positive': {k: v.squeeze(0) for k, v in positive_encoding.items()},\n",
    "            'negative': {k: v.squeeze(0) for k, v in negative_encoding.items()}\n",
    "        }\n",
    "\n",
    "class DPRTestDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        question = item['question']\n",
    "        rows = item['rows']\n",
    "        \n",
    "        question_encoding = self.tokenizer(question, truncation=True, padding='max_length', \n",
    "                                           max_length=self.max_length, return_tensors='pt')\n",
    "        \n",
    "        row_encodings = [self.tokenizer(' '.join(str(v) for v in row.values()), \n",
    "                                        truncation=True, padding='max_length',\n",
    "                                        max_length=self.max_length, return_tensors='pt')\n",
    "                         for row in rows]\n",
    "        \n",
    "        return {\n",
    "            'question': {k: v.squeeze(0) for k, v in question_encoding.items()},\n",
    "            'rows': [{k: v.squeeze(0) for k, v in encoding.items()} for encoding in row_encodings],\n",
    "            'table_id': item['table_id'],\n",
    "            'correct_index': item['correct_row_index']\n",
    "        }\n",
    "\n",
    "# 单元格 4: 编码和训练函数\n",
    "def encode_data(model, dataset, device):\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    question_embeddings = []\n",
    "    positive_embeddings = []\n",
    "    negative_embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Encoding\"):\n",
    "            q_emb = model.encode_question(\n",
    "                batch['question']['input_ids'].to(device),\n",
    "                batch['question']['attention_mask'].to(device)\n",
    "            )\n",
    "            p_emb = model.encode_passage(\n",
    "                batch['positive']['input_ids'].to(device),\n",
    "                batch['positive']['attention_mask'].to(device)\n",
    "            )\n",
    "            n_emb = model.encode_passage(\n",
    "                batch['negative']['input_ids'].to(device),\n",
    "                batch['negative']['attention_mask'].to(device)\n",
    "            )\n",
    "            \n",
    "            question_embeddings.extend(q_emb.cpu().numpy())\n",
    "            positive_embeddings.extend(p_emb.cpu().numpy())\n",
    "            negative_embeddings.extend(n_emb.cpu().numpy())\n",
    "    \n",
    "    return np.array(question_embeddings), np.array(positive_embeddings), np.array(negative_embeddings)\n",
    "\n",
    "def train(model, train_dataloader, optimizer, scheduler, device, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            q_emb = model.encode_question(\n",
    "                batch['question']['input_ids'].to(device),\n",
    "                batch['question']['attention_mask'].to(device)\n",
    "            )\n",
    "            p_emb = model.encode_passage(\n",
    "                batch['positive']['input_ids'].to(device),\n",
    "                batch['positive']['attention_mask'].to(device)\n",
    "            )\n",
    "            n_emb = model.encode_passage(\n",
    "                batch['negative']['input_ids'].to(device),\n",
    "                batch['negative']['attention_mask'].to(device)\n",
    "            )\n",
    "            \n",
    "            # 计算相似度\n",
    "            pos_score = F.cosine_similarity(q_emb, p_emb)\n",
    "            neg_score = F.cosine_similarity(q_emb, n_emb)\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = F.margin_ranking_loss(pos_score, neg_score, \n",
    "                                         torch.ones_like(pos_score), \n",
    "                                         margin=0.1)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # 梯度裁剪\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# 单元格 5: 测试函数\n",
    "def test_model(model, test_dataset, device):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in range(len(test_dataset)):\n",
    "            batch = test_dataset[idx]\n",
    "            q_emb = model.encode_question(\n",
    "                batch['question']['input_ids'].unsqueeze(0).to(device),\n",
    "                batch['question']['attention_mask'].unsqueeze(0).to(device)\n",
    "            )\n",
    "            \n",
    "            row_embeddings = []\n",
    "            for row in batch['rows']:\n",
    "                r_emb = model.encode_passage(\n",
    "                    row['input_ids'].unsqueeze(0).to(device),\n",
    "                    row['attention_mask'].unsqueeze(0).to(device)\n",
    "                )\n",
    "                row_embeddings.append(r_emb)\n",
    "            \n",
    "            row_embeddings = torch.cat(row_embeddings, dim=0)\n",
    "            \n",
    "            similarities = torch.matmul(q_emb, row_embeddings.t()).squeeze()\n",
    "            predicted_index = similarities.argmax().item()\n",
    "            \n",
    "            results.append({\n",
    "                'table_id': batch['table_id'],\n",
    "                'label': predicted_index,\n",
    "                'row': test_dataset.data[idx]['rows'][predicted_index],\n",
    "                'answer_node': batch['correct_index']\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 单元格 6: 数据加载和模型初始化\n",
    "# 加载训练数据\n",
    "print(f\"Loading prepared training data from {PREPARED_DATA_FILE}\")\n",
    "with open(PREPARED_DATA_FILE, \"r\") as f:\n",
    "    training_data = json.load(f)\n",
    "print(f\"Loaded {len(training_data)} prepared training samples.\")\n",
    "\n",
    "# 加载测试数据\n",
    "print(f\"Loading test data from {TEST_DATA_FILE}\")\n",
    "with open(TEST_DATA_FILE, \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "print(f\"Loaded {len(test_data)} test samples.\")\n",
    "\n",
    "# 初始化tokenizer和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = DPRModel('bert-base-uncased').to(device)\n",
    "\n",
    "# 单元格 7: 训练模型\n",
    "# 创建训练数据集和数据加载器\n",
    "train_dataset = DPRDataset(training_data, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# 设置优化器和学习率调度器\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "# 训练模型\n",
    "print(\"Starting model training...\")\n",
    "train(model, train_dataloader, optimizer, scheduler, device, epochs=5)\n",
    "\n",
    "# 单元格 8: 编码数据\n",
    "# 编码训练数据\n",
    "print(\"Encoding training data...\")\n",
    "question_embeddings, positive_embeddings, negative_embeddings = encode_data(model, train_dataset, device)\n",
    "\n",
    "# 保存编码后的向量\n",
    "np.save(os.path.join(BASE_PATH, \"question_embeddings.npy\"), question_embeddings)\n",
    "np.save(os.path.join(BASE_PATH, \"positive_embeddings.npy\"), positive_embeddings)\n",
    "np.save(os.path.join(BASE_PATH, \"negative_embeddings.npy\"), negative_embeddings)\n",
    "\n",
    "print(\"Encoded data saved.\")\n",
    "\n",
    "# 展示编码结果的形状\n",
    "print(f\"Question embeddings shape: {question_embeddings.shape}\")\n",
    "print(f\"Positive embeddings shape: {positive_embeddings.shape}\")\n",
    "print(f\"Negative embeddings shape: {negative_embeddings.shape}\")\n",
    "\n",
    "# 展示一个样本的编码结果\n",
    "print(\"\\nSample embeddings:\")\n",
    "print(f\"Question: {question_embeddings[0][:5]}...\")  # 只显示前5个元素\n",
    "print(f\"Positive: {positive_embeddings[0][:5]}...\")\n",
    "print(f\"Negative: {negative_embeddings[0][:5]}...\")\n",
    "\n",
    "# 单元格 9: 测试模型\n",
    "# 创建测试数据集\n",
    "test_dataset = DPRTestDataset(test_data, tokenizer)\n",
    "\n",
    "# 测试模型\n",
    "test_results = test_model(model, test_dataset, device)\n",
    "\n",
    "# 输出测试结果\n",
    "print(\"\\nTest Results:\")\n",
    "for result in test_results:\n",
    "    print(f\"Table ID: {result['table_id']}\")\n",
    "    print(f\"Predicted Row: {result['label']}\")\n",
    "    print(f\"Correct Row: {result['answer_node']}\")\n",
    "    print(f\"Retrieved Row Content: {result['row']}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# 保存测试结果\n",
    "with open(\"predict_result.json\", \"w\") as f:\n",
    "    json.dump(test_results, f, indent=2)\n",
    "print(\"Test results saved to predict_result.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19a5105-6199-453f-aaf6-330ae3a9d0ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
