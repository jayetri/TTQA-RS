{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f65186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "028281c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--option\", default=\"cot\", type=str)\n",
    "parser.add_argument(\"--model\", default=\"llama2-70b\", type=str, help=\" \")\n",
    "parser.add_argument(\"--start\", default=0, type=int)\n",
    "parser.add_argument(\"--end\", default=None, type=int)\n",
    "parser.add_argument(\n",
    "    \"--temperature\",\n",
    "    type=float,\n",
    "    default=0.5,\n",
    "    help=\"temperature of 0 implies greedy sampling.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--traced_json_file\",\n",
    "    default=r\"traced.json\",#traced file\n",
    "    type=str,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--tables_json_file\",\n",
    "    default=r\"tables.json\",#table files\n",
    "    type=str,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--topk_path\",\n",
    "    default=r\"request_tok\",#text files\n",
    "    \n",
    "    type=str,\n",
    ")\n",
    "\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e279553",
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstration = {}\n",
    "demonstration[\"none\"] = \"\"\n",
    "with open(\"examples/fullmodel_direct_2shot.json\", \"r\") as f:\n",
    "    demonstration[\"direct\"] = json.load(f)\n",
    "with open(\"examples/fullmodel_cot_2shot.json\", \"r\") as f:\n",
    "    demonstration[\"cot\"] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dca55ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(args):\n",
    "    # Load traced JSON file\n",
    "    data_test_traced = json.load(open(args.traced_json_file, \"r\"))\n",
    "    data_list = []\n",
    "    for sample in tqdm(data_test_traced[args.start:args.end]):\n",
    "        table_id = sample[\"table_id\"]\n",
    "        question_data = None\n",
    "        for q_data in questions_data:\n",
    "            if q_data['table_id'] == table_id:\n",
    "                question_data = q_data\n",
    "                break\n",
    "        if question_data is None:\n",
    "            print(f\"No question data found for {table_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Read JSON file from tables_tok\n",
    "        try:\n",
    "            tables_tok_path = f\"{table_id}.json\"  # put your traced table link\n",
    "            with open(tables_tok_path, 'r') as f:\n",
    "                table_data = json.load(f)\n",
    "        except Exception:\n",
    "            print(f\"The file {table_id} does not exist.\")\n",
    "            continue\n",
    "\n",
    "        question_type = question_data['type']\n",
    "        if question_type == 'bridge':\n",
    "            # Get the index of the most relevant row\n",
    "            row_index = question_data['row_pre']\n",
    "            relevant_rows = [table_data['data'][row_index]]\n",
    "        elif question_type == 'comparison':\n",
    "            # Get the indices of all rows with relevance less than or equal to 1.0\n",
    "            row_pre_logits = question_data['row_pre_logit']\n",
    "            relevant_rows = [table_data['data'][i] for i, logit in enumerate(row_pre_logits) if logit <= 1.0]\n",
    "        else:\n",
    "            print(f\"Unknown question type: {question_type}\")\n",
    "            continue\n",
    "\n",
    "        # Read text data\n",
    "        try:\n",
    "            text_file = os.path.join(args.text_path, f\"{table_id}.json\")\n",
    "            with open(text_file, \"r\") as f:\n",
    "                text_data = json.load(f)\n",
    "        except Exception:\n",
    "            print(f\"The file {text_file} does not exist.\")\n",
    "            continue\n",
    "            \n",
    "        question_text = sample[\"question\"]\n",
    "        answer_text = sample[\"pred\"]\n",
    "        \n",
    "        # Extract wiki links from nodes and target\n",
    "        wikis = [\n",
    "            node[2]\n",
    "            for node in sample[\"nodes\"]\n",
    "            if node[2] is not None and node[2].startswith(\"/wiki\")\n",
    "        ]\n",
    "        \n",
    "        target_wiki = sample[\"target\"][2]\n",
    "        if target_wiki and target_wiki.startswith(\"/wiki\"):\n",
    "            wikis.append(target_wiki)\n",
    "        \n",
    "        # Get the corresponding text for each wiki link\n",
    "        wiki_text = \"\"\n",
    "        if wikis:\n",
    "            wiki_lines = [text_data.get(wiki, \"\") for wiki in wikis]\n",
    "            wiki_text = \"\\n\".join(wiki_lines)\n",
    "        \n",
    "        # Create a DataFrame from the table data\n",
    "        df = pd.DataFrame(\n",
    "            [tuple(zip(*row))[0] for row in table_data[\"data\"]],\n",
    "            columns=list(zip(*table_data[\"header\"]))[0],\n",
    "        )\n",
    "\n",
    "        data_list.append({\n",
    "            \"table_id\": table_id,\n",
    "            \"question\": question_text,\n",
    "            \"answer\": answer_text,\n",
    "            \"table\": df,\n",
    "            \"wiki\": wiki_text,\n",
    "            \"title\": table_data[\"title\"],\n",
    "            \"intro\": table_data[\"intro\"]\n",
    "        })\n",
    "\n",
    "    return data_list\n",
    "\n",
    "# Load questions data\n",
    "questions_path = \"test.json\"  # put text answer here\n",
    "with open(questions_path, 'r') as f:\n",
    "    questions_data = json.load(f)\n",
    "\n",
    "def df_format(data):\n",
    "    try:\n",
    "        formatted_str = \" | \".join(data.columns) + \"\\n\"\n",
    "        for _, row in data.iterrows():\n",
    "            row_str = \" | \".join([str(row[col]) for col in data.columns])\n",
    "            formatted_str += row_str + \"\\n\"\n",
    "        return formatted_str\n",
    "    except Exception as e:\n",
    "        #print(f\"Error formatting table: {data}, error: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e532ff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model or API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "56254a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2556"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "run_count = 0\n",
    "\n",
    "subquestion_file = f\"outputs/subquestion_s{args.start}_e{args.end}_{args.option}_{args.model}_{run_count}.json\"\n",
    "subquestion_fw = open(subquestion_file, \"w\")\n",
    "\n",
    "tmp = {\"demonstration\": demonstration[args.option]}\n",
    "subquestion_fw.write(json.dumps(tmp) + \"\\n\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a55fc01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 166.88it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "data_list = read_data(args)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ab6dc51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.75s/it]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "with open('outputs/question_test_s300_eNone.json', 'r') as f:\n",
    "    subquestion_data = [json.loads(line) for line in f]\n",
    "        \n",
    "with open('outputs/summary_s300_e600.json', 'r') as f:\n",
    "    summary_data = [json.loads(line) for line in f]\n",
    "\n",
    "with open('subquestion_spacy_s300_e600_test.txt', 'r', encoding='utf-8') as f:\n",
    "    entity_data = [line.strip() for line in f]\n",
    "\n",
    "\n",
    "question_idx = 0\n",
    "\n",
    "for entry, entity_entry,subquestion_entry, summary_entry in zip(tqdm(data_list), entity_data, subquestion_data, summary_data):\n",
    "    summary = summary_entry.get('summary', '')\n",
    "    subquestion = subquestion_entry.get('response', '')\n",
    "\n",
    "    prompt = demonstration[args.option] + '\\n\\n'\n",
    "    #### Formalizing the k-shot demonstration. #####\n",
    "    prompt += f'Read the table and text regarding \"{entry[\"title\"]}\" to answer the question.\\n\\n'\n",
    "    prompt += df_format(entry['table']) + '\\n'\n",
    "\n",
    "    if entry['wiki']:\n",
    "        prompt += \"Text:\" + '\\n' + entry['wiki'] + '\\n\\n'\n",
    "    prompt += 'Summary: ' + summary + '\\n\\n'\n",
    "    prompt += 'The answer should be a/an ' + entity_entry + '\\n\\n'\n",
    "    prompt += 'Lets think step by step, to answer the question: ' + subquestion + '\\nAnswer:'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    question_idx += 1\n",
    "    response_raw = query({'inputs': prompt})\n",
    "    try:\n",
    "        response = response_raw[0].get('generated_text', '').split('\\nAnswer:')[3].split('Reasoning process')[0].strip()\n",
    "    except KeyError:\n",
    "        response = ''\n",
    "\n",
    "    response = response.split('\\n')[0].strip()\n",
    "\n",
    "    tmp = {\n",
    "        \"sub_question\": subquestion,\n",
    "        \"entity\":entity_entry,\n",
    "        \"sub_answer\": response,\n",
    "        \"table_id\": entry[\"table_id\"],\n",
    "    }\n",
    "\n",
    "    subquestion_fw.write(json.dumps(tmp) + \"\\n\")\n",
    "\n",
    "subquestion_fw.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d556af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6209"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d_%H_%M\")\n",
    "answer_fw = open(f\"outputs/answer_s{args.start}_e{args.end}_{args.option}_{args.model}_{dt_string}.json\", \"w\",)\n",
    "tmp = {\"demonstration\": demonstration[args.option]}\n",
    "answer_fw.write(json.dumps(tmp) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "363c25f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|████████████████████████████████████████████████████▉                         | 407/600 [00:00<00:00, 1032.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file Homicide:_Life_on_the_Street_0 does not exist.\n",
      "The file Ebertfest:_Roger_Ebert's_Film_Festival_6 does not exist.\n",
      "The file List_of_National_Treasures_of_Japan_(writings:_Chinese_books)_0 does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 600/600 [00:00<00:00, 972.05it/s]\n"
     ]
    }
   ],
   "source": [
    "data_list = read_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da853735",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 597/597 [18:35<00:00,  1.87s/it]\n"
     ]
    }
   ],
   "source": [
    "with open('outputs/subquestion.json', 'r') as f: #load your subquestion answer here\n",
    "    subquestion_data = [json.loads(line) for line in f]\n",
    "    \n",
    "with open('spacy_test.txt', 'r', encoding='utf-8') as f: #load your entity for full question here\n",
    "    entity_data = [line.strip() for line in f]\n",
    "    \n",
    "with open('summary.json', 'r') as f: #load your summary here\n",
    "    summary_data = [json.loads(line) for line in f]\n",
    "    \n",
    "# Iterate over data_list and evidence_data simultaneously\n",
    "for entry, subquestion_entry, entity_entry, summary_entry in zip(tqdm(data_list), subquestion_data, entity_data, summary_data):\n",
    "    question = entry['question']\n",
    "    answer = entry['answer']\n",
    "    table_id = entry['table_id']\n",
    "    subanswer = subquestion_entry.get('sub_answer', '')  # Use .get() to handle KeyError\n",
    "    subquestion = subquestion_entry.get('sub_question', '')\n",
    "    subquestion_table_id = subquestion_entry.get('table_id', '')  # Get evidence table_id\n",
    "    summary = summary_entry.get('summary', '')\n",
    "    \n",
    "\n",
    "    # Check if evidence table_id matches the entry table_id\n",
    "    if subquestion_table_id != table_id:\n",
    "        print(f\"Warning: Table ID mismatch for question '{question}'.\")\n",
    "        # Optionally, you can choose to skip this entry or handle it differently\n",
    "\n",
    "    #### Formalizing the k-shot demonstration. #####\n",
    "    prompt = demonstration[args.option] + '\\n\\n'\n",
    "    prompt += f'Read the following table, text and summary regarding \"{entry[\"title\"]}\":'+'and answer the question.\\n\\n'\n",
    "    prompt += df_format(entry['table']) + '\\n'\n",
    "\n",
    "    if entry['wiki']:\n",
    "        prompt += \"Text: \" + '\\n' + entry['wiki'] + '\\n\\n'\n",
    "    prompt +=  \"Summary: \" + summary+ '\\n\\n'\n",
    "    # Add evidence to the prompt\n",
    "    prompt += \"A subquestion as hint : \" + subquestion + \"\\nThe answer of this hint subquestion: \" + subanswer + '\\n\\n'\n",
    "    prompt += \"Using exact the same words from the text and the table as answers can lead to better accuracy. Simplify your answer to an entity \"+ '\\n\\n'\n",
    "    prompt += 'Let us think step by step, and answer the question: ' + question \n",
    "    prompt += '\\nAnswer:'\n",
    "    response_raw = query({'inputs': prompt})\n",
    "\n",
    "    try:\n",
    "        response = response_raw[0].get('generated_text', '').split('\\nAnswer:')[5].split('Reasoning process')[0].strip()\n",
    "    except KeyError:\n",
    "        response = ''\n",
    "\n",
    "    response = response.split('\\n')[0].strip()\n",
    "\n",
    "    tmp = {\n",
    "        \"question\": question,\n",
    "        \"response\": response,\n",
    "        \"answer\": answer,\n",
    "        \"entity\":entity_entry,\n",
    "        \"table_id\": entry[\"table_id\"],\n",
    "        \"sub_answer\": subanswer\n",
    "    }\n",
    "\n",
    "    answer_fw.write(json.dumps(tmp) + \"\\n\")\n",
    "\n",
    "answer_fw.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6492c988-ee25-426d-b9a5-2a100f79009a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I'll provide you with two demonstrations. You need to combine information from both the table and the text.\n",
      "\n",
      "First: \n",
      "Read the table below regarding the \"2006 League of Ireland Premier Division\". \n",
      "\n",
      "Team | Manager | Main sponsor | Kit supplier | Stadium | Capacity\n",
      "Bray Wanderers | Eddie Gormley | Slevin Group | Adidas | Carlisle Grounds | 7,000\n",
      "\n",
      "Text:\n",
      "The Carlisle Grounds is a football stadium in Bray , County Wicklow , Ireland . Situated directly behind the Bray D.A.R.T . station , it is home to Bray Wanderers A.F.C . Its current capacity is roughly 4,000 .\n",
      "\n",
      "Subquestion: What is the home stadium of the Bray Wanderers of 2006 League of Ireland?\n",
      "The answer of Subquestion : Carlisle Grounds\n",
      "\n",
      "Let's thinks step by step, to answer the question : The home stadium of the Bray Wanderers of 2006 League of Ireland is situated behind what station ?\n",
      "The answer should be a : geopolitical entity\n",
      "\n",
      "Answer: Bray D.A.R.T station\n",
      "The resoning process of this question: \n",
      "Let's think step by step, From the breakdown question answer we know their stadium is listed as \"Carlisle Grounds\".The additional text information mentions that the Carlisle Grounds is situated behind a station in Bray, County Wicklow, Ireland. Putting both pieces of information together, we can conclude that the home stadium of Bray Wanderers, the Carlisle Grounds, is situated behind a station in Bray, County Wicklow. The text specifically mentions the Bray D.A.R.T station.Therefore, the answer to the question is \"Bray D.A.R.T station\".\n",
      "\n",
      "Second:\n",
      "Read the table blow regarding \"List of Rangers F.C. records and statistics\" to answer the following question.\n",
      "\n",
      "# | Player | To | Fee | Date\n",
      "3 | Jean-Alain Boumsong | Newcastle United | £8,000,000 | 1 January 2005\n",
      "\n",
      "Text:\n",
      "Jean-Alain Boumsong Somkong ( born 14 December 1979 ) is a former professional football defender , including French international . He is known for his physical strength , pace and reading of the game .\n",
      "\n",
      "Subquestion: Who is the third highest paid Rangers F.C . player\n",
      "The answer of Subquestion : Jean-Alain Boumsong\n",
      "\n",
      "Let's thinks step by step, to answer the question : When was the third highest paid Rangers F.C . player born ?\n",
      "\n",
      "Answer: 14 December 1979.\n",
      "The resoning process of this question: \n",
      "From the table the third highest paid Rangers F.C . player is Jean-Alain Boumsong, and we look for the information in text, we know he was born 14 December 1979.\n",
      "\n",
      "Now, it's your turn to read the following table and text and answer the question.\n",
      "\n",
      "\n",
      "Read the following table and text regarding \"West Bromwich Albion F.C.\":and answer the question.\n",
      "\n",
      "Name | Years | Apps | Goals | Position\n",
      "Bryan Robson | 1974-81 | 249 | 46 | Central midfielder\n",
      "\n",
      "Text: \n",
      "Bryan Robson OBE ( born 11 January 1957 ) is an English football manager and former player . Born in Chester-le-Street , County Durham , he began his career with West Bromwich Albion in 1972 before moving to Manchester United in 1981 , where he became the longest serving captain in the club 's history and won two Premier League winners ' medals , three FA Cups , two FA Charity Shields and a European Cup Winners ' Cup . In August 2011 , Robson was voted as the greatest ever Manchester United player in a poll of the club 's former players as part of a new book , 19 , released to celebrate the club 's record-breaking 19th league title . Robson represented England on 90 occasions between 1980 and 1991 , making him , at the time , the fifth most capped England player . His goalscoring tally of 26 placed him eighth on the list at the time . Robson captained his country 65 times , with only Bobby Moore and Billy Wright having captained England on more occasions . Robson is also known by the nicknames Robbo and Captain Marvel . Bobby Robson stated that Robson was , along with Alan Shearer and Kevin Beattie , the best British player he ever worked with . Robson began his management career as a player-manager with Middlesbrough in 1994 , retiring from playing in 1997 . In seven years as Middlesbrough manager , he guided them to three Wembley finals , which were all lost , and earned them promotion to the Premier League on two occasions . Between 1994 and 1996 , he also served as assistant coach to England manager Terry Venables , which included Euro 96 . He later returned to West Bromwich Albion for two years as manager , helping them become the first top division team in 14 years to avoid relegation after being bottom of the league table on Christmas Day .\n",
      "\n",
      "Summary: The table provides information about the Gatorade Player of the Year award winner for the year 2012, Molly Seidel, from Hartland, Wisconsin, who attended Notre Dame college. The text provides additional information about Hartland, describing it as a village in Wisconsin, along the Bark River, with a population of 9,110 as of 2010, and situated as a suburb of Milwaukee.\n",
      "\n",
      "Subquestion: What is hometown of the 2012 Gatorade Player of the Year ?\n",
      "The answer of subquestion: Hartland, WI\n",
      "\n",
      "Using exactly the same word from the text and table as answer can achieve better correct rate. Simplify your answer to a/an :date\n",
      "\n",
      "Lets think step by step, and answer question: What are the goals of the athlete who initiated his management career as a player-manager with Middlesbrough in 1994 ?\n",
      "Answer: 46\n",
      "Process: \n",
      "From the table the position of Bryan Robson, it is a central midfielder. Text provides the information about his career. We know that he was with West Bromwich Albion and Manchester United. At the same time, his playing statistics are in the table. So from the table, we know his appearances and his goals. In this case, the answer should be Goal(no Goal for this one).|year or years |apps |positions |Goals\n",
      "Bryan Rob\n"
     ]
    }
   ],
   "source": [
    "print(response_raw[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f21225-f092-49f4-8b2b-a087131f0991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
