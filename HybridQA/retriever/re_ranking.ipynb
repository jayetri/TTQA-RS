{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7ddfd3f-1d6a-48f4-994e-4bcac59b7c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DPRQuestionEncoder, DPRContextEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoderTokenizer\n",
    "\n",
    "# question encodera and tokenizer\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "\n",
    "# context encoder and tokenizer\n",
    "context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8bf52a-04ec-407a-ac01-8fb165ea34e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from transformers import pipeline, BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Load BART model and tokenizer\n",
    "model_name = \"facebook/bart-large\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Create text generation pipeline\n",
    "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def clean_generated_text(generated_text):\n",
    "    \"\"\"Remove irrelevant prefix content\"\"\"\n",
    "    prefixes = [\n",
    "        \"Generate a description for this table row:\",\n",
    "        \"Description for this table row:\",\n",
    "        \"a description for this table row:\",\n",
    "        \"table row:\"\n",
    "    ]\n",
    "    for prefix in prefixes:\n",
    "        if generated_text.lower().startswith(prefix.lower()):\n",
    "            generated_text = generated_text[len(prefix):].strip()\n",
    "    return generated_text\n",
    "\n",
    "def generate_sentence_for_row(row, headers):\n",
    "    \"\"\"Generate a descriptive sentence for a table row\"\"\"\n",
    "    # Extract useful table information, ignore empty values or invalid content\n",
    "    row_text = \", \".join([f\"{header}: {value[0]}\" for header, value in zip(headers, row) if value[0]])\n",
    "    \n",
    "    # Ensure table data is reasonable\n",
    "    if not row_text:\n",
    "        return \"No relevant data\"\n",
    "    \n",
    "    # Generate prompt for descriptive sentence\n",
    "    prompt = f\"Generate a descriptive sentence for this table row: {row_text}\"\n",
    "    \n",
    "    try:\n",
    "        # Generate concise descriptive text\n",
    "        output = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "        generated_text = output[0]['generated_text'].strip()\n",
    "        \n",
    "        # Post-processing: remove unnecessary prefixes\n",
    "        cleaned_text = clean_generated_text(generated_text)\n",
    "        \n",
    "        return cleaned_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating sentence: {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "def process_table(json_data):\n",
    "    \"\"\"Process the entire table, generate descriptive sentences for each row\"\"\"\n",
    "    headers = [header[0] for header in json_data['header']]\n",
    "    sentences = []\n",
    "    for row in json_data['data']:\n",
    "        sentence = generate_sentence_for_row(row, headers)\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "def process_files(input_json_path, table_tok_dir, output_dir, num_files=None):\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Read initial JSON file\n",
    "    with open(input_json_path, 'r') as file:\n",
    "        input_data = json.load(file)\n",
    "    \n",
    "    # Get all table_ids\n",
    "    table_ids = [item['table_id'] for item in input_data]\n",
    "    \n",
    "    # If number of files is specified, only process that many\n",
    "    if num_files is not None:\n",
    "        table_ids = table_ids[:num_files]\n",
    "    \n",
    "    for table_id in table_ids:\n",
    "        # Construct input JSON file path\n",
    "        input_file_path = os.path.join(table_tok_dir, f\"{table_id}.json\")\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not os.path.exists(input_file_path):\n",
    "            print(f\"File not found: {input_file_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Read JSON file\n",
    "        with open(input_file_path, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        # Process table\n",
    "        sentences = process_table(json_data)\n",
    "        \n",
    "        # Write results to file\n",
    "        output_file_path = os.path.join(output_dir, f\"{table_id}.txt\")\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "            formatted_sentences = [f'\"{sentence}\"' for sentence in sentences if sentence]\n",
    "            file.write(',\\n'.join(formatted_sentences))\n",
    "        \n",
    "        print(f\"Summary has been written to {output_file_path}\")\n",
    "\n",
    "input_json_path = \"dev.json\"\n",
    "table_tok_dir = \"WikiTables-WithLinks-master/tables_tok\"  # Folder containing JSON files with table data\n",
    "output_dir = \"row_summary\"  # Output directory\n",
    "    \n",
    "num_files = input(\"Enter the number of files to process (press Enter for all): \").strip()\n",
    "num_files = int(num_files) if num_files else None\n",
    "    \n",
    "process_files(input_json_path, table_tok_dir, output_dir, num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e11f8f0-6a30-4347-b3d0-95e3ac655b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most relevant document is: 'Scott Overall from the United Kingdom placed 5th with a time of 2:10:55' with similarity score 0.5861918926239014\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# question encoder\n",
    "question = \"What place was achieved by the person who finished the Berlin marathon in 2 hours, 13 minutes, ans 32 seconds in 2011 the first time he competed in a marathon ?\"\n",
    "question_inputs = question_tokenizer(question, return_tensors='pt')\n",
    "question_embedding = question_encoder(**question_inputs).pooler_output\n",
    "\n",
    "\n",
    "contexts = [\n",
    "'Patrick Makau Musyoki from Kenya finished in 1st place with a time of 2:3:38',\n",
    "'Stephen Kwelio Chemlany from Kenya came in 2nd place with a time of 2:7:55',\n",
    " \"Edwin Kimaiyo from Kenya secured 3rd place with a time of 2:9:50\",\n",
    "\"Felix Limo from Kenya finished 4th with a time of 2:10:38\",\n",
    "    \"Scott Overall from the United Kingdom placed 5th with a time of 2:10:55\",\n",
    "    \"Ricardo Serrano from Spain took 6th place with a time of 2:13:32\",\n",
    "    \"Simon Munyutu from France came in 8th place with a time of 2:14:20\",\n",
    "    \"Driss El Himer from France secured 9th place with a time of 2:14:46\",\n",
    "    \"Hendrick Ramaala from South Africa finished in 10th place with a time of 2:16:0\",\n",
    "]\n",
    "\n",
    "context_embeddings = []\n",
    "for context in contexts:\n",
    "    context_inputs = context_tokenizer(context, return_tensors='pt')\n",
    "    context_embedding = context_encoder(**context_inputs).pooler_output\n",
    "    context_embeddings.append(context_embedding)\n",
    "\n",
    "similarities = [torch.nn.functional.cosine_similarity(question_embedding, context_embedding).item() for context_embedding in context_embeddings]\n",
    "\n",
    "most_similar_idx = similarities.index(max(similarities))\n",
    "print(f\"The most relevant document is: '{contexts[most_similar_idx]}' with similarity score {similarities[most_similar_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45231587-1ee3-4c9e-9e9f-e8df460e757a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at deepset/roberta-base-squad2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved context: Walter Payton, ranked second, played for the Chicago Bears (1975-1987). He had 3,838 carries for 16,726 yards, averaging 4.4 yards per carry.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import re\n",
    "\n",
    "# 使用适合问答任务的模型\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def encode_text(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "question = \"What is the middle name of the player with the second most National Football League career rushing yards?\"\n",
    "question_embedding = encode_text(question)\n",
    "\n",
    "keywords = [\"second\", \"most\", \"rushing yards\"]\n",
    "\n",
    "def retrieve_context(question, contexts, top_k=6):\n",
    "\n",
    "    context_scores = []\n",
    "    for idx, context in enumerate(contexts):\n",
    "        context_embedding = encode_text(context)\n",
    "        similarity = torch.nn.functional.cosine_similarity(question_embedding, context_embedding).item()\n",
    "        keyword_score = sum(keyword.lower() in context.lower() for keyword in keywords)\n",
    "        context_scores.append((idx, similarity + keyword_score * 0.1))  # 将相似度和关键词得分结合\n",
    "    top_k_indices = sorted(range(len(context_scores)), key=lambda i: context_scores[i][1], reverse=True)[:top_k]\n",
    "    \n",
    "    candidates = [contexts[i] for i in top_k_indices]\n",
    "    final_scores = []\n",
    "    for candidate in candidates:\n",
    "        # 提取排名和码数\n",
    "        rank_match = re.search(r'^(\\d+)', candidate)\n",
    "        yards_match = re.search(r'\\| (\\d{1,3}(,\\d{3})*) \\|', candidate)\n",
    "        if rank_match and yards_match:\n",
    "            rank = int(rank_match.group(1))\n",
    "            yards = int(yards_match.group(1).replace(',', ''))\n",
    "            final_scores.append(rank_score + yards_score)\n",
    "        else:\n",
    "            final_scores.append(0)\n",
    "    \n",
    "    best_index = final_scores.index(max(final_scores))\n",
    "    return candidates[best_index]\n",
    "\n",
    "retrieved_context = retrieve_context(question, contexts)\n",
    "print(\"Retrieved context:\", retrieved_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a68033b7-9ddb-4667-b028-6caa2e657938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 relevant document: 'Walter Payton, ranked second, played for the Chicago Bears (1975-1987). He had 3,838 carries for 16,726 yards, averaging 4.4 yards per carry.' with similarity score -0.0035529606975615025\n",
      "Top 2 relevant document: 'Frank Gore, ranked third, played for the San Francisco 49ers (2005-2014), Indianapolis Colts (2015-2017), Miami Dolphins (2018), and Buffalo Bills (2019-present). He had 3,548 carries for 15,347 yards, averaging 4.3 yards per carry.' with similarity score -0.007335161790251732\n",
      "Top 3 relevant document: 'Emmitt Smith, ranked first, played for the Dallas Cowboys (1990-2002) and Arizona Cardinals (2003-2004). He had 4,409 carries for 18,355 yards, averaging 4.2 yards per carry.' with similarity score -0.03946688771247864\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "similarities = [torch.nn.functional.cosine_similarity(question_embedding, context_embedding).item() for context_embedding in context_embeddings]\n",
    "\n",
    "# Get the indices of the top n highest similarity scores\n",
    "top_n_indices = heapq.nlargest(9, range(len(similarities)), key=similarities.__getitem__)\n",
    "\n",
    "# Print the top n most relevant documents and their similarity scores\n",
    "for i, idx in enumerate(top_n_indices, 1):\n",
    "    print(f\"Top {i} relevant document: '{contexts[idx]}' with similarity score {similarities[idx]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4132e4d5-1e50-4136-a317-f1fc995e338d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted keywords: ['Which', 'middle', 'name', 'the', 'player', 'with', 'second', 'most', 'career', 'rushing']\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# 加载T5模型和tokenizer\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# 输入问题\n",
    "question = \"What is the middle name of the player with the second most National Football League career rushing yards?\"\n",
    "\n",
    "# 准备T5输入，使用关键词提取任务\n",
    "input_text = \"extract keywords: \" + question\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# 生成关键词，调整生成参数以限制输出长度\n",
    "outputs = model.generate(\n",
    "    inputs['input_ids'], \n",
    "    max_length=16,  # 限制最大生成长度\n",
    "    num_beams=5, \n",
    "    early_stopping=True, \n",
    "    no_repeat_ngram_size=1  # 避免重复\n",
    ")\n",
    "\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "cleaned_text = generated_text.replace(\"extract keywords:\", \"\").strip()  \n",
    "\n",
    "keywords = [word.strip() for word in cleaned_text.split() if len(word) > 2]  # 过滤长度小于2的词\n",
    "\n",
    "print(\"Extracted keywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dba9f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, DPRQuestionEncoder, DPRContextEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoderTokenizer\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Initialize DPR models and tokenizers\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "\n",
    "# Load RoBERTa model for keyword and rule processing\n",
    "roberta_model_name = \"deepset/roberta-base-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(roberta_model_name)\n",
    "roberta_model = AutoModel.from_pretrained(roberta_model_name)\n",
    "\n",
    "# Load T5 model and tokenizer\n",
    "t5_model_name = \"t5-small\"\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
    "\n",
    "# DPR encoding function\n",
    "def encode_text_dpr(text, encoder, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        return encoder(**inputs).pooler_output\n",
    "\n",
    "# Keyword extraction function\n",
    "def extract_keywords(question):\n",
    "    input_text = \"extract keywords: \" + question\n",
    "    inputs = t5_tokenizer(input_text, return_tensors=\"pt\", padding=True)\n",
    "    outputs = t5_model.generate(\n",
    "        inputs['input_ids'], \n",
    "        max_length=16,\n",
    "        num_beams=5, \n",
    "        early_stopping=True, \n",
    "        no_repeat_ngram_size=1\n",
    "    )\n",
    "    generated_text = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    cleaned_text = generated_text.replace(\"extract keywords:\", \"\").strip()\n",
    "    keywords = [word.strip() for word in cleaned_text.split() if len(word) > 2]\n",
    "    return keywords\n",
    "\n",
    "# Stage 1: Initial DPR retrieval\n",
    "def initial_retrieval(question, contexts, top_k=8):\n",
    "    question_embedding = encode_text_dpr(question, question_encoder, question_tokenizer)\n",
    "    context_scores = []\n",
    "    \n",
    "    for idx, context in enumerate(contexts):\n",
    "        context_embedding = encode_text_dpr(context, context_encoder, context_tokenizer)\n",
    "        similarity_dpr = F.cosine_similarity(question_embedding, context_embedding).item()\n",
    "        context_scores.append((idx, similarity_dpr))\n",
    "    \n",
    "    top_k_indices = sorted(range(len(context_scores)), key=lambda i: context_scores[i][1], reverse=True)[:top_k]\n",
    "    return [contexts[i] for i in top_k_indices], top_k_indices\n",
    "\n",
    "def dpr_fine_ranking(question, candidates, keywords):\n",
    "    question_embedding = encode_text_dpr(question, question_encoder, question_tokenizer)\n",
    "    final_scores = []\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        candidate_embedding = encode_text_dpr(candidate, context_encoder, context_tokenizer)\n",
    "        dpr_similarity = F.cosine_similarity(question_embedding, candidate_embedding).item()\n",
    "        \n",
    "        keyword_score = sum(keyword.lower() in candidate.lower() for keyword in keywords)\n",
    "        \n",
    "        final_score = dpr_similarity + keyword_score * 0.1  # Keyword score weight 0.1, adjustable\n",
    "        final_scores.append((candidate, final_score))\n",
    "    \n",
    "    # Sort by score in descending order\n",
    "    sorted_results = sorted(final_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top three results\n",
    "    return sorted_results[:3]\n",
    "\n",
    "def retrieve_context(question, contexts, top_k=8):\n",
    "    keywords = extract_keywords(question)\n",
    "    candidates, _ = initial_retrieval(question, contexts, top_k=top_k)\n",
    "    top_contexts = dpr_fine_ranking(question, candidates, keywords)\n",
    "    return top_contexts\n",
    "\n",
    "def retrieve_and_save(question, contexts, file_name, output_dir=\"row_retrieve\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    top_contexts = retrieve_context(question, contexts)\n",
    "    \n",
    "    output_data = {\n",
    "        \"table_id\": os.path.splitext(file_name)[0],\n",
    "        \"question\": question\n",
    "    }\n",
    "    \n",
    "    for i, (context, score) in enumerate(top_contexts, 1):\n",
    "        output_data[f\"retrieve_content{i}\"] = context\n",
    "        output_data[f\"number{i}\"] = contexts.index(context) + 1  # +1 because row numbers usually start from 1\n",
    "        output_data[f\"score{i}\"] = score\n",
    "\n",
    "    output_file = os.path.join(output_dir, f\"{os.path.splitext(file_name)[0]}_retrieve.json\")\n",
    "    \n",
    "    # Write to JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Results have been saved to {output_file}\")\n",
    "\n",
    "question = \"What was the nickname of the gold medal winner in the men's heavyweight greco-roman wrestling event of the 1932 Summer Olympics?\"\n",
    "file_name = \"Sweden_at_the_1932_Summer_Olympics_0.txt\"\n",
    "file_path = os.path.join(\"row_summary\", file_name)\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    contexts = file.readlines()\n",
    "contexts = [line.strip() for line in contexts if line.strip()]\n",
    "\n",
    "retrieve_and_save(question, contexts, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35ddb7c5-a35c-4e32-b131-f797e5842e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of files to process (press Enter for all):  20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary has been written to row_summary/List_of_National_Football_League_rushing_yards_leaders_0.txt\n",
      "Summary has been written to row_summary/Sweden_at_the_1932_Summer_Olympics_0.txt\n",
      "Summary has been written to row_summary/2004_United_States_Grand_Prix_0.txt\n",
      "Summary has been written to row_summary/List_of_museums_in_Atlanta_0.txt\n",
      "Summary has been written to row_summary/2011_Berlin_Marathon_0.txt\n",
      "Summary has been written to row_summary/List_of_football_stadiums_in_Paraguay_0.txt\n",
      "Summary has been written to row_summary/List_of_wealthiest_non-inflated_historical_figures_13.txt\n",
      "Summary has been written to row_summary/1929_International_Cross_Country_Championships_0.txt\n",
      "Summary has been written to row_summary/List_of_Somali_cities_by_population_0.txt\n",
      "Summary has been written to row_summary/Flora_and_fauna_of_Madhya_Pradesh_0.txt\n",
      "Summary has been written to row_summary/List_of_Mohun_Bagan_A.C._managers_0.txt\n",
      "Summary has been written to row_summary/List_of_Indonesian_dishes_3.txt\n",
      "Summary has been written to row_summary/List_of_best-selling_books_4.txt\n",
      "Summary has been written to row_summary/List_of_the_oldest_buildings_in_Maryland_0.txt\n",
      "Summary has been written to row_summary/List_of_the_mothers_of_the_Ottoman_Sultans_0.txt\n",
      "Summary has been written to row_summary/Ben_Foster_(actor)_0.txt\n",
      "Summary has been written to row_summary/Brazil_at_the_2004_Summer_Olympics_0.txt\n",
      "Summary has been written to row_summary/Australia_at_the_Winter_Olympics_1.txt\n",
      "Summary has been written to row_summary/129th_Ohio_General_Assembly_2.txt\n",
      "Summary has been written to row_summary/List_of_Virtual_Console_games_for_Nintendo_3DS_(Japan)_10.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from transformers import pipeline, BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Load BART model and tokenizer\n",
    "model_name = \"facebook/bart-large\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Create text generation pipeline\n",
    "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def clean_generated_text(generated_text):\n",
    "    \"\"\"Remove irrelevant prefix content\"\"\"\n",
    "    prefixes = [\n",
    "        \"Generate a description for this table row:\",\n",
    "        \"Description for this table row:\",\n",
    "        \"a description for this table row:\",\n",
    "        \"table row:\"\n",
    "    ]\n",
    "    for prefix in prefixes:\n",
    "        if generated_text.lower().startswith(prefix.lower()):\n",
    "            generated_text = generated_text[len(prefix):].strip()\n",
    "    return generated_text\n",
    "\n",
    "def generate_sentence_for_row(row, headers):\n",
    "    \"\"\"Generate a descriptive sentence for a table row\"\"\"\n",
    "    # Extract useful table information, ignore empty or invalid content\n",
    "    row_text = \", \".join([f\"{header}: {value[0]}\" for header, value in zip(headers, row) if value[0]])\n",
    "    \n",
    "    # Ensure table data is reasonable\n",
    "    if not row_text:\n",
    "        return \"No relevant data\"\n",
    "    \n",
    "    # Generate prompt for descriptive sentence\n",
    "    prompt = f\"Generate a description for this table row: {row_text}\"\n",
    "    \n",
    "    try:\n",
    "        # Generate concise descriptive text\n",
    "        output = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "        generated_text = output[0]['generated_text'].strip()\n",
    "        \n",
    "        # Post-processing: remove unnecessary prefixes\n",
    "        cleaned_text = clean_generated_text(generated_text)\n",
    "        return cleaned_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating sentence: {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "def process_table(json_data):\n",
    "    \"\"\"Process the entire table, generating descriptive sentences for each row\"\"\"\n",
    "    headers = [header[0] for header in json_data['header']]\n",
    "    sentences = []\n",
    "    for row in json_data['data']:\n",
    "        sentence = generate_sentence_for_row(row, headers)\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "def process_files(input_json_path, table_tok_dir, output_dir, num_files=None):\n",
    "    \"\"\"Process specified number of files\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Read initial JSON file\n",
    "    with open(input_json_path, 'r') as file:\n",
    "        input_data = json.load(file)\n",
    "    \n",
    "    # Get all table_ids\n",
    "    table_ids = [item['table_id'] for item in input_data]\n",
    "    \n",
    "    # If number of files is specified, only process that many\n",
    "    if num_files is not None:\n",
    "        table_ids = table_ids[:num_files]\n",
    "    \n",
    "    for table_id in table_ids:\n",
    "        # Construct input JSON file path\n",
    "        input_file_path = os.path.join(table_tok_dir, f\"{table_id}.json\")\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not os.path.exists(input_file_path):\n",
    "            print(f\"File not found: {input_file_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Read JSON file\n",
    "        with open(input_file_path, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        # Process table\n",
    "        sentences = process_table(json_data)\n",
    "        \n",
    "        # Write results to file\n",
    "        output_file_path = os.path.join(output_dir, f\"{table_id}.txt\")\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "            formatted_sentences = [f'\"{sentence}\"' for sentence in sentences if sentence]\n",
    "            file.write(',\\n'.join(formatted_sentences))\n",
    "        \n",
    "        print(f\"Summary has been written to {output_file_path}\")\n",
    "\n",
    "input_json_path = \"released_data/dev_traced.json\"\n",
    "table_tok_dir = \"WikiTables-WithLinks-master/tables_tok\"  # Folder containing table data JSON files\n",
    "output_dir = \"row_summary\"  # Output directory\n",
    "\n",
    "num_files = input(\"Enter the number of files to process (press Enter for all): \").strip()\n",
    "num_files = int(num_files) if num_files else None\n",
    "\n",
    "process_files(input_json_path, table_tok_dir, output_dir, num_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e069cfa6-6a5c-4ff3-81d2-347de899af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, DPRQuestionEncoder, DPRContextEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoderTokenizer\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import os\n",
    "\n",
    "class FlexibleRetrievalSystem:\n",
    "    def __init__(self):\n",
    "        self.question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "        self.context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "        self.question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "        self.context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "\n",
    "        roberta_model_name = \"deepset/roberta-base-squad2\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(roberta_model_name)\n",
    "        self.roberta_model = AutoModel.from_pretrained(roberta_model_name)\n",
    "\n",
    "        t5_model_name = \"t5-small\"\n",
    "        self.t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
    "        self.t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
    "\n",
    "    def encode_text_dpr(self, text, encoder, tokenizer):\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            return encoder(**inputs).pooler_output\n",
    "\n",
    "    def extract_keywords(self, question):\n",
    "        input_text = \"extract keywords: \" + question\n",
    "        inputs = self.t5_tokenizer(input_text, return_tensors=\"pt\", padding=True)\n",
    "        outputs = self.t5_model.generate(\n",
    "            inputs['input_ids'], \n",
    "            max_length=16,\n",
    "            num_beams=5, \n",
    "            early_stopping=True, \n",
    "            no_repeat_ngram_size=1\n",
    "        )\n",
    "        generated_text = self.t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        cleaned_text = generated_text.replace(\"extract keywords:\", \"\").strip()\n",
    "        keywords = [word.strip() for word in cleaned_text.split() if len(word) > 2]\n",
    "        return keywords\n",
    "\n",
    "    def initial_retrieval(self, question, contexts, top_k=8):\n",
    "        question_embedding = self.encode_text_dpr(question, self.question_encoder, self.question_tokenizer)\n",
    "        context_scores = []\n",
    "        \n",
    "        for idx, context in enumerate(contexts):\n",
    "            context_embedding = self.encode_text_dpr(context, self.context_encoder, self.context_tokenizer)\n",
    "            similarity_dpr = F.cosine_similarity(question_embedding, context_embedding).item()\n",
    "            context_scores.append((idx, similarity_dpr))\n",
    "        \n",
    "        top_k_indices = sorted(range(len(context_scores)), key=lambda i: context_scores[i][1], reverse=True)[:top_k]\n",
    "        return [contexts[i] for i in top_k_indices], top_k_indices\n",
    "\n",
    "    def dpr_fine_ranking(self, question, candidates, keywords):\n",
    "        question_embedding = self.encode_text_dpr(question, self.question_encoder, self.question_tokenizer)\n",
    "        final_scores = []\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            candidate_embedding = self.encode_text_dpr(candidate, self.context_encoder, self.context_tokenizer)\n",
    "            dpr_similarity = F.cosine_similarity(question_embedding, candidate_embedding).item()\n",
    "            \n",
    "            keyword_score = sum(keyword.lower() in candidate.lower() for keyword in keywords)\n",
    "            \n",
    "            final_score = dpr_similarity + keyword_score * 0.1  # Keyword score weight 0.1, adjustable\n",
    "            final_scores.append((candidate, final_score))\n",
    "        \n",
    "        sorted_results = sorted(final_scores, key=lambda x: x[1], reverse=True)\n",
    "        return sorted_results[:3]\n",
    "\n",
    "    def retrieve_context(self, question, contexts, top_k=8):\n",
    "        keywords = self.extract_keywords(question)\n",
    "        candidates, _ = self.initial_retrieval(question, contexts, top_k=top_k)\n",
    "        top_contexts = self.dpr_fine_ranking(question, candidates, keywords)\n",
    "        return top_contexts\n",
    "\n",
    "    def retrieve_and_save(self, question, contexts, file_name, output_dir=\"row_retrieve\"):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        top_contexts = self.retrieve_context(question, contexts)\n",
    "        \n",
    "        output_data = {\n",
    "            \"table_id\": os.path.splitext(file_name)[0],\n",
    "            \"question\": question\n",
    "        }\n",
    "        \n",
    "        for i, (context, score) in enumerate(top_contexts, 1):\n",
    "            # Remove extra quotes and backslashes from the context\n",
    "            cleaned_context = context.strip('\"').replace('\\\\\"', '\"')\n",
    "            output_data[f\"retrieve_content{i}\"] = cleaned_context\n",
    "            output_data[f\"number{i}\"] = contexts.index(context) + 1\n",
    "            output_data[f\"score{i}\"] = score\n",
    "\n",
    "        output_file = os.path.join(output_dir, f\"{os.path.splitext(file_name)[0]}_retrieve.json\")\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Results have been saved to {output_file}\")\n",
    "\n",
    "\n",
    "def process_questions_from_json(json_file_path, retrieval_system, contexts_dir=\"row_summary\", output_dir=\"row_retrieve\"):\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        questions_data = json.load(f)\n",
    "\n",
    "    for item in questions_data:\n",
    "        question_id = item['question_id']\n",
    "        question = item['question']\n",
    "        table_id = item['table_id']\n",
    "        \n",
    "        file_name = f\"{table_id}.txt\"\n",
    "        file_path = os.path.join(contexts_dir, file_name)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            contexts = file.readlines()\n",
    "        contexts = [line.strip() for line in contexts if line.strip()]\n",
    "        \n",
    "        retrieval_system.retrieve_and_save(question, contexts, file_name, output_dir)\n",
    "        print(f\"Processed question ID: {question_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c73f754d-0533-4bc7-90f4-0dc829ceb492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at deepset/roberta-base-squad2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of questions to process (press Enter for all):  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been saved to row_retrieve/List_of_National_Football_League_rushing_yards_leaders_0_retrieve.json\n",
      "Processed question ID: 00153f694413a536\n",
      "Results have been saved to row_retrieve/Sweden_at_the_1932_Summer_Olympics_0_retrieve.json\n",
      "Processed question ID: 001a9923f31d6a91\n",
      "Results have been saved to row_retrieve/2004_United_States_Grand_Prix_0_retrieve.json\n",
      "Processed question ID: 0035c791af3d9666\n",
      "Processed 3 questions.\n"
     ]
    }
   ],
   "source": [
    "def process_questions_from_json(json_file_path, retrieval_system, contexts_dir=\"row_summary\", output_dir=\"row_retrieve\", num_questions=None):\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        questions_data = json.load(f)\n",
    "\n",
    "    if num_questions is not None:\n",
    "        questions_data = questions_data[:num_questions]\n",
    "\n",
    "    for item in questions_data:\n",
    "        question_id = item['question_id']\n",
    "        question = item['question']\n",
    "        table_id = item['table_id']\n",
    "        \n",
    "        file_name = f\"{table_id}.txt\"\n",
    "        file_path = os.path.join(contexts_dir, file_name)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            contexts = file.readlines()\n",
    "        contexts = [line.strip() for line in contexts if line.strip()]\n",
    "        \n",
    "        retrieval_system.retrieve_and_save(question, contexts, file_name, output_dir)\n",
    "        print(f\"Processed question ID: {question_id}\")\n",
    "\n",
    "    print(f\"Processed {len(questions_data)} questions.\")\n",
    "\n",
    "\n",
    "retrieval_system = FlexibleRetrievalSystem()\n",
    "json_file_path = \"released_data/dev_traced.json\"\n",
    "    \n",
    "    # 添加用户输入来决定处理的问题数量\n",
    "num_questions = input(\"Enter the number of questions to process (press Enter for all): \").strip()\n",
    "num_questions = int(num_questions) if num_questions else None\n",
    "    \n",
    "process_questions_from_json(json_file_path, retrieval_system, num_questions=num_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a8a45d-4af5-4b31-99f4-1f2002baf3e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
