{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f65186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "028281c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--option\", default=\"cot\", type=str)\n",
    "parser.add_argument(\"--model\", default=\"llama2-70b\", type=str, help=\" \")\n",
    "parser.add_argument(\"--start\", default=0, type=int)\n",
    "parser.add_argument(\"--end\", default=None, type=int)\n",
    "parser.add_argument(\n",
    "    \"--temperature\",\n",
    "    type=float,\n",
    "    default=0.5,\n",
    "    help=\"temperature of 0 implies greedy sampling.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--traced_json_file\",\n",
    "    default=r\"traced.json\",#traced file\n",
    "    type=str,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--tables_json_file\",\n",
    "    default=r\"tables.json\",#table files\n",
    "    type=str,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--topk_path\",\n",
    "    default=r\"request_tok\",#text files\n",
    "    \n",
    "    type=str,\n",
    ")\n",
    "\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e279553",
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstration = {}\n",
    "demonstration[\"none\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dca55ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(args):\n",
    "    # Read traced JSON file\n",
    "    data_test_traced = json.load(open(args.traced_json_file, \"r\"))\n",
    "    data_list = []\n",
    "    for sample in tqdm(data_test_traced[args.start:args.end]):\n",
    "        table_id = sample[\"table_id\"]\n",
    "        question_data = None\n",
    "        for q_data in questions_data:\n",
    "            if q_data['table_id'] == table_id:\n",
    "                question_data = q_data\n",
    "                break\n",
    "        if question_data is None:\n",
    "            print(f\"No question data found for {table_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Read JSON file from tables_tok\n",
    "        try:\n",
    "            tables_tok_path = f\"{table_id}.json\"  # put your traced table link\n",
    "            with open(tables_tok_path, 'r') as f:\n",
    "                table_data = json.load(f)\n",
    "        except Exception:\n",
    "            print(f\"The file {table_id} does not exist.\")\n",
    "            continue\n",
    "\n",
    "        question_type = question_data['type']\n",
    "        if question_type == 'bridge':\n",
    "            # Get index of the most relevant row\n",
    "            row_index = question_data['row_pre']\n",
    "            relevant_rows = [table_data['data'][row_index]]\n",
    "        elif question_type == 'comparison':\n",
    "            # Get indices of all rows with relevance less than 1.0\n",
    "            row_pre_logits = question_data['row_pre_logit']\n",
    "            relevant_rows = [table_data['data'][i] for i, logit in enumerate(row_pre_logits) if logit < 1.0]\n",
    "        else:\n",
    "            print(f\"Unknown question type: {question_type}\")\n",
    "            continue\n",
    "\n",
    "        # Read text data\n",
    "        try:\n",
    "            text_file = os.path.join(args.text_path, f\"{table_id}.json\")\n",
    "            with open(text_file, \"r\") as f:\n",
    "                text_data = json.load(f)\n",
    "        except Exception:\n",
    "            print(f\"The file {text_file} does not exist.\")\n",
    "            continue\n",
    "            \n",
    "        question_text = sample[\"question\"]\n",
    "        answer_text = sample[\"answer-text\"]\n",
    "        \n",
    "\n",
    "        # Extract wiki links from nodes\n",
    "        wikis = [\n",
    "            node[2]\n",
    "            for node in sample.get(\"answer-node\", [])\n",
    "            if node[2] is not None and node[2].startswith(\"/wiki\")\n",
    "        ]\n",
    "\n",
    "        # Get the corresponding text for each wiki link\n",
    "        wiki_texts = []\n",
    "        for wiki_link in wikis:\n",
    "            wiki_text = text_data.get(wiki_link, \"\")\n",
    "            wiki_texts.append(wiki_text)\n",
    "\n",
    "        # Concatenate wiki_texts into a string, separating each wiki's text content with newline\n",
    "        wiki_text = \"\\n\".join(wiki_texts)\n",
    "\n",
    "\n",
    "        # Create a DataFrame from the table data\n",
    "        df = pd.DataFrame(\n",
    "            [tuple(zip(*row))[0] for row in table_data[\"data\"]],\n",
    "            columns=list(zip(*table_data[\"header\"]))[0],\n",
    "        )\n",
    "\n",
    "        # Create a DataFrame from the relevant rows\n",
    "        try:\n",
    "            # Flatten the table header\n",
    "            flattened_header = [col[0] for col in table_data[\"header\"]]\n",
    "            # Flatten the relevant rows\n",
    "            flattened_data = [[cell if not isinstance(cell, list) else cell[0] for cell in row] for row in relevant_rows]\n",
    "            df = pd.DataFrame(flattened_data, columns=flattened_header)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating DataFrame for {table_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        data_list.append({\n",
    "            \"table_id\": table_id,\n",
    "            \"question\": question_text,\n",
    "            \"answer\": answer_text,\n",
    "            \"table\": df,\n",
    "            \"wiki\": wiki_text,\n",
    "            \"title\": table_data[\"title\"],\n",
    "            \"intro\": table_data[\"intro\"]\n",
    "        })\n",
    "\n",
    "    return data_list\n",
    "\n",
    "# Read questions data from the development set's standard answers\n",
    "questions_path = \"dev\"\n",
    "with open(questions_path, 'r') as f:\n",
    "    questions_data = json.load(f)\n",
    "\n",
    "def df_format(data):\n",
    "    try:\n",
    "        formatted_str = \" | \".join(data.columns) + \"\\n\"\n",
    "        for _, row in data.iterrows():\n",
    "            row_str = \" | \".join([str(row[col]) for col in data.columns])\n",
    "            formatted_str += row_str + \"\\n\"\n",
    "        return formatted_str\n",
    "    except Exception as e:\n",
    "        #print(f\"Error formatting table: {data}, error: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e532ff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model or API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5c31237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnow = datetime.now()\\ndt_string = now.strftime(\"%d_%H_%M\")\\nfw = open(f\"outputs/response_s{args.start}_e{args.end}_{args.option}_{args.model}_{dt_string}.json\", \"w\",)\\ntmp = {\"demonstration\": demonstration[args.option]}\\nfw.write(json.dumps(tmp) + \"\\n\")\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d_%H_%M\")\n",
    "fw = open(f\"outputs/response_s{args.start}_e{args.end}_{args.option}_{args.model}_{dt_string}.json\", \"w\",)\n",
    "tmp = {\"demonstration\": demonstration[args.option]}\n",
    "fw.write(json.dumps(tmp) + \"\\n\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfc0446b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.30it/s]\n"
     ]
    }
   ],
   "source": [
    "data_list = read_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4863c0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.78s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = \"outputs/summary\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "for entry in tqdm(data_list):\n",
    "    question = entry['question']\n",
    "    answer = entry['answer']\n",
    "\n",
    "    #### Formalizing the k-shot demonstration. #####\n",
    "    prompt = demonstration[args.option] + '\\n\\n'\n",
    "    prompt += f'Read the table and text regarding \"{entry[\"title\"]}\" and create a summary.\\n\\n'\n",
    "    prompt += df_format(entry['table']) + '\\n'\n",
    "    \n",
    "    if entry['wiki']:\n",
    "        prompt += \"Text\" + '\\n' + entry['wiki'] + '\\n\\n'\n",
    "\n",
    "    prompt += 'Summarize the given table and text. ' + '\\nSummary:'\n",
    "\n",
    "    response_raw = query({'inputs': prompt})\n",
    "    try:\n",
    "        response = response_raw[0].get('generated_text', '').split('\\nSummary:')[1].split('Reasoning process')[0].strip()\n",
    "    except KeyError:\n",
    "        response = ''\n",
    "\n",
    "    response = response.split('\\n')[0].strip()\n",
    "\n",
    "    output_file_path = os.path.join(output_dir, f\"{entry['table_id']}.txt\")\n",
    "\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as fw:\n",
    "        fw.write(f\"Prompt:\\n{prompt}\\n\\n\")\n",
    "        fw.write(f\"Summary: {response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0aa6e1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [09:06<00:00,  2.73s/it]\n"
     ]
    }
   ],
   "source": [
    "for entry in tqdm(data_list):\n",
    "    question = entry['question']\n",
    "    answer = entry['answer']\n",
    "\n",
    "    #### Formalizing the k-shot demonstration. #####\n",
    "    prompt = demonstration[args.option] + '\\n\\n'\n",
    "    prompt += f'Read the table and text regarding \"{entry[\"title\"]}\" and create a summary.\\n\\n'\n",
    "    #prompt += f\"This is the introduction of the table:\" + '\\n' + entry['intro'] + '\\n\\n'\n",
    "    prompt += df_format(entry['table']) + '\\n'\n",
    "    \n",
    "    if entry['wiki']:\n",
    "        prompt += \"Text\" + '\\n' + entry['wiki'] + '\\n\\n'\n",
    "\n",
    "    prompt += 'Summarize the given table and text. ' + '\\nSummary:'\n",
    "\n",
    "    response_raw = query({'inputs': prompt})\n",
    "    try:\n",
    "        response = response_raw[0].get('generated_text', '').split('\\nSummary:')[1].split('Reasoning process')[0].strip()\n",
    "    except KeyError:\n",
    "        response = ''\n",
    "\n",
    "    response = response.split('\\n')[0].strip()\n",
    "\n",
    "    tmp = {\n",
    "        \"question\": question,\n",
    "        \"summary\": response,\n",
    "        \"table_id\": entry[\"table_id\"],\n",
    "    }\n",
    "\n",
    "    fw.write(json.dumps(tmp) + \"\\n\")\n",
    "\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6492c988-ee25-426d-b9a5-2a100f79009a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Rate limit reached. You reached PRO hourly usage limit. Use Inference Endpoints (dedicated) to scale your endpoint.'}\n"
     ]
    }
   ],
   "source": [
    "print(response_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f21225-f092-49f4-8b2b-a087131f0991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
