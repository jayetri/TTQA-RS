{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f65186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "028281c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--option\", default=\"cot\", type=str)\n",
    "parser.add_argument(\"--model\", default=\"llama2-70b\", type=str, help=\" \")\n",
    "parser.add_argument(\"--start\", default=0, type=int)\n",
    "parser.add_argument(\"--end\", default=None, type=int)\n",
    "parser.add_argument(\n",
    "    \"--temperature\",\n",
    "    type=float,\n",
    "    default=0.5,\n",
    "    help=\"temperature of 0 implies greedy sampling.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--traced_json_file\",\n",
    "    default=r\"traced.json\",#traced file\n",
    "    type=str,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--tables_json_file\",\n",
    "    default=r\"tables.json\",#table files\n",
    "    type=str,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--topk_path\",\n",
    "    default=r\"request_tok\",#text files\n",
    "    \n",
    "    type=str,\n",
    ")\n",
    "\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e279553",
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstration = {}\n",
    "demonstration[\"none\"] = \"\"\n",
    "with open(\"examples/fullmodel_direct_2shot.json\", \"r\") as f:\n",
    "    demonstration[\"direct\"] = json.load(f)\n",
    "with open(\"examples/fullmodel_cot_2shot.json\", \"r\") as f:\n",
    "    demonstration[\"cot\"] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dca55ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(args):\n",
    "    # Read traced JSON file\n",
    "    data_test_traced = json.load(open(args.traced_json_file, \"r\"))\n",
    "    data_list = []\n",
    "    for sample in tqdm(data_test_traced[args.start:args.end]):\n",
    "        table_id = sample[\"table_id\"]\n",
    "        question_data = None\n",
    "        for q_data in questions_data:\n",
    "            if q_data['table_id'] == table_id:\n",
    "                question_data = q_data\n",
    "                break\n",
    "        if question_data is None:\n",
    "            print(f\"No question data found for {table_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Read JSON file from tables_tok\n",
    "        try:\n",
    "            tables_tok_path = f\"{table_id}.json\"  # traced table link\n",
    "            with open(tables_tok_path, 'r') as f:\n",
    "                table_data = json.load(f)\n",
    "        except Exception:\n",
    "            print(f\"The file {table_id} does not exist.\")\n",
    "            continue\n",
    "\n",
    "        question_type = question_data['type']\n",
    "        if question_type == 'bridge':\n",
    "            # Get index of the most relevant row\n",
    "            row_index = question_data['row_pre']\n",
    "            relevant_rows = [table_data['data'][row_index]]\n",
    "        elif question_type == 'comparison':\n",
    "            # Get indices of all rows with relevance less than 1.0\n",
    "            row_pre_logits = question_data['row_pre_logit']\n",
    "            relevant_rows = [table_data['data'][i] for i, logit in enumerate(row_pre_logits) if logit < 1.0]\n",
    "        else:\n",
    "            print(f\"Unknown question type: {question_type}\")\n",
    "            continue\n",
    "\n",
    "        # Read text data\n",
    "        try:\n",
    "            text_file = os.path.join(args.text_path, f\"{table_id}.json\")\n",
    "            with open(text_file, \"r\") as f:\n",
    "                text_data = json.load(f)\n",
    "        except Exception:\n",
    "            print(f\"The file {text_file} does not exist.\")\n",
    "            continue\n",
    "            \n",
    "        question_text = sample[\"question\"]\n",
    "        answer_text = sample[\"answer-text\"]\n",
    "        \n",
    "\n",
    "        # Extract wiki links from nodes\n",
    "        wikis = [\n",
    "            node[2]\n",
    "            for node in sample.get(\"answer-node\", [])\n",
    "            if node[2] is not None and node[2].startswith(\"/wiki\")\n",
    "        ]\n",
    "\n",
    "        # Get the corresponding text for each wiki link\n",
    "        wiki_texts = []\n",
    "        for wiki_link in wikis:\n",
    "            wiki_text = text_data.get(wiki_link, \"\")\n",
    "            wiki_texts.append(wiki_text)\n",
    "\n",
    "        # Concatenate wiki_texts into a string, separating each wiki's text content with newline\n",
    "        wiki_text = \"\\n\".join(wiki_texts)\n",
    "\n",
    "\n",
    "        # Create a DataFrame from the table data\n",
    "        df = pd.DataFrame(\n",
    "            [tuple(zip(*row))[0] for row in table_data[\"data\"]],\n",
    "            columns=list(zip(*table_data[\"header\"]))[0],\n",
    "        )\n",
    "\n",
    "        # Create a DataFrame from the relevant rows\n",
    "        try:\n",
    "            # Flatten the table header\n",
    "            flattened_header = [col[0] for col in table_data[\"header\"]]\n",
    "            # Flatten the relevant rows\n",
    "            flattened_data = [[cell if not isinstance(cell, list) else cell[0] for cell in row] for row in relevant_rows]\n",
    "            df = pd.DataFrame(flattened_data, columns=flattened_header)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating DataFrame for {table_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        data_list.append({\n",
    "            \"table_id\": table_id,\n",
    "            \"question\": question_text,\n",
    "            \"answer\": answer_text,\n",
    "            \"table\": df,\n",
    "            \"wiki\": wiki_text,\n",
    "            \"title\": table_data[\"title\"],\n",
    "            \"intro\": table_data[\"intro\"]\n",
    "        })\n",
    "\n",
    "    return data_list\n",
    "\n",
    "# Read questions data from the development set's standard answers\n",
    "questions_path = \"dev\"\n",
    "with open(questions_path, 'r') as f:\n",
    "    questions_data = json.load(f)\n",
    "\n",
    "def df_format(data):\n",
    "    try:\n",
    "        formatted_str = \" | \".join(data.columns) + \"\\n\"\n",
    "        for _, row in data.iterrows():\n",
    "            row_str = \" | \".join([str(row[col]) for col in data.columns])\n",
    "            formatted_str += row_str + \"\\n\"\n",
    "        return formatted_str\n",
    "    except Exception as e:\n",
    "        #print(f\"Error formatting table: {data}, error: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e532ff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model or API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56254a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2556"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_count = 0\n",
    "\n",
    "subquestion_file = f\"outputs/subquestion_s{args.start}_e{args.end}_{args.option}_{args.model}_{run_count}.json\"\n",
    "subquestion_fw = open(subquestion_file, \"w\")\n",
    "\n",
    "tmp = {\"demonstration\": demonstration[args.option]}\n",
    "subquestion_fw.write(json.dumps(tmp) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a55fc01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|████████████████████████████▋                                                 | 221/600 [00:00<00:00, 1101.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file Ebertfest:_Roger_Ebert's_Film_Festival_3 does not exist.\n",
      "The file Ebertfest:_Roger_Ebert's_Film_Festival_16 does not exist.\n",
      "The file List_of_National_Treasures_of_Japan_(writings:_Japanese_books)_0 does not exist.\n",
      "The file Brad_Nelson_(Magic:_The_Gathering_player)_0 does not exist.\n",
      "The file Looney_Tunes_Golden_Collection:_Volume_3_0 does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 600/600 [00:00<00:00, 1046.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file Looney_Tunes_Golden_Collection:_Volume_5_1 does not exist.\n",
      "The file Ebertfest:_Roger_Ebert's_Film_Festival_13 does not exist.\n",
      "The file List_of_microcars_by_country_of_origin:_J_0 does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_list = read_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6dc51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('question_dev.json', 'r', encoding='utf-8') as f:\n",
    "    subquestion_data = [json.loads(line) for line in f]\n",
    "        \n",
    "with open('summary.json', 'r') as f: #load your summary here\n",
    "    summary_data = [json.loads(line) for line in f]\n",
    "\n",
    "with open('subquestion_entity.txt', 'r', encoding='utf-8') as f: #load your entity for subquestion here\n",
    "    entity_data = [line.strip() for line in f]\n",
    "\n",
    "\n",
    "question_idx = 0\n",
    "\n",
    "for entry, entity_entry,subquestion_entry, summary_entry in zip(tqdm(data_list), entity_data, subquestion_data, summary_data):\n",
    "    summary = summary_entry.get('summary', '')\n",
    "    subquestion = subquestion_entry.get('response', '')\n",
    "\n",
    "    prompt = demonstration[args.option] + '\\n\\n'\n",
    "    #### Formalizing the k-shot demonstration. #####\n",
    "    prompt += f'Read the table and text regarding \"{entry[\"title\"]}\" to answer the question.\\n\\n'\n",
    "    prompt += df_format(entry['table']) + '\\n'\n",
    "\n",
    "    if entry['wiki']:\n",
    "        prompt += \"Text:\" + '\\n' + entry['wiki'] + '\\n\\n'\n",
    "    #prompt += 'Summary: ' + summary + '\\n\\n'\n",
    "    prompt += 'The answer should be a/an ' + entity_entry + '\\n\\n'\n",
    "    prompt += 'Let us think step by step, and answer the question: ' + subquestion + '\\nAnswer:'\n",
    "\n",
    "    # 处理问题和答案...\n",
    "\n",
    "    # 更新问题索引\n",
    "    question_idx += 1\n",
    "    response_raw = query({'inputs': prompt})\n",
    "    try:\n",
    "        response = response_raw[0].get('generated_text', '').split('\\nAnswer:')[3].split('Reasoning process')[0].strip()\n",
    "    except KeyError:\n",
    "        response = ''\n",
    "\n",
    "    response = response.split('\\n')[0].strip()\n",
    "\n",
    "    tmp = {\n",
    "        \"sub_question\": subquestion,\n",
    "        \"entity\":entity_entry,\n",
    "        \"sub_answer\": response,\n",
    "        \"table_id\": entry[\"table_id\"],\n",
    "    }\n",
    "\n",
    "    subquestion_fw.write(json.dumps(tmp) + \"\\n\")\n",
    "\n",
    "subquestion_fw.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d556af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5647"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d_%H_%M\")\n",
    "answer_fw = open(f\"outputs/answer_s{args.start}_e{args.end}_{args.option}_{args.model}_{dt_string}.json\", \"w\",)\n",
    "tmp = {\"demonstration\": demonstration[args.option]}\n",
    "answer_fw.write(json.dumps(tmp) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "363c25f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█████▎                                                                           | 39/600 [00:02<00:29, 18.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file Ebertfest:_Roger_Ebert's_Film_Festival_3 does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▎                                                                         | 54/600 [00:03<00:26, 20.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file Ebertfest:_Roger_Ebert's_Film_Festival_16 does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|████████████████████▊                                                           | 156/600 [00:09<00:20, 21.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file List_of_National_Treasures_of_Japan_(writings:_Japanese_books)_0 does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|████████████████████████▌                                                       | 184/600 [00:10<00:23, 18.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file Brad_Nelson_(Magic:_The_Gathering_player)_0 does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████▍                                               | 243/600 [00:14<00:20, 17.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file Looney_Tunes_Golden_Collection:_Volume_3_0 does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|███████████████████████████████████████████████████████████████████████▎        | 535/600 [00:33<00:03, 20.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file Looney_Tunes_Golden_Collection:_Volume_5_1 does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████████████▏       | 541/600 [00:33<00:02, 21.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file Ebertfest:_Roger_Ebert's_Film_Festival_13 does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██████████████████████████████████████████████████████████████████████████████▌ | 589/600 [00:36<00:00, 20.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file List_of_microcars_by_country_of_origin:_J_0 does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 600/600 [00:36<00:00, 16.39it/s]\n"
     ]
    }
   ],
   "source": [
    "data_list = read_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da853735",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 592/592 [22:05<00:00,  2.24s/it]\n"
     ]
    }
   ],
   "source": [
    "with open('outputs/subquestion.json', 'r') as f: #load your subquestion answer here\n",
    "    subquestion_data = [json.loads(line) for line in f]\n",
    "    \n",
    "with open('spacy_dev.txt', 'r', encoding='utf-8') as f: #load your entity for full question here\n",
    "    entity_data = [line.strip() for line in f]\n",
    "    \n",
    "with open('summary.json', 'r') as f: #load your summary here\n",
    "    summary_data = [json.loads(line) for line in f]\n",
    "    \n",
    "# Iterate over data_list and evidence_data simultaneously\n",
    "for entry, subquestion_entry, entity_entry, summary_entry in zip(tqdm(data_list), subquestion_data, entity_data, summary_data):\n",
    "    question = entry['question']\n",
    "    answer = entry['answer']\n",
    "    table_id = entry['table_id']\n",
    "    subanswer = subquestion_entry.get('sub_answer', '')  # Use .get() to handle KeyError\n",
    "    subquestion = subquestion_entry.get('sub_question', '')\n",
    "    subquestion_table_id = subquestion_entry.get('table_id', '')  # Get evidence table_id\n",
    "    summary = summary_entry.get('summary', '')\n",
    "    \n",
    "\n",
    "    # Check if evidence table_id matches the entry table_id\n",
    "    if subquestion_table_id != table_id:\n",
    "        print(f\"Warning: Table ID mismatch for question '{question}'.\")\n",
    "        # Optionally, you can choose to skip this entry or handle it differently\n",
    "\n",
    "    #### Formalizing the k-shot demonstration. #####\n",
    "    prompt = demonstration[args.option] + '\\n\\n'\n",
    "    prompt += f'Read the following table and text regarding \"{entry[\"title\"]}\":'+'and answer the question.\\n\\n'\n",
    "    prompt += df_format(entry['table']) + '\\n'\n",
    "\n",
    "    if entry['wiki']:\n",
    "        prompt += \"Text: \" + '\\n' + entry['wiki'] + '\\n\\n'\n",
    "    #prompt +=  \"Summary: \" + summary+ '\\n\\n'\n",
    "    # Add evidence to the prompt\n",
    "    prompt += \"Subquestion as hint: \" + subquestion + \"\\nThe answer of subquestion: \" + subanswer + '\\n\\n'\n",
    "    prompt += \"Using exactly the same word from the text and table as answer can achieve better correct rate. Simplify your answer to a/an:\" + entity_entry + '\\n\\n'\n",
    "    prompt += 'Lets think step by step, and answer the question: ' + question \n",
    "    prompt += '\\nAnswer:'\n",
    "    response_raw = query({'inputs': prompt})\n",
    "\n",
    "    try:\n",
    "        response = response_raw[0].get('generated_text', '').split('\\nAnswer:')[4].split('Reasoning process')[0].strip()\n",
    "    except KeyError:\n",
    "        response = ''\n",
    "\n",
    "    response = response.split('\\n')[0].strip()\n",
    "\n",
    "    tmp = {\n",
    "        \"question\": question,\n",
    "        \"response\": response,\n",
    "        \"answer\": answer,\n",
    "        \"entity\":entity_entry,\n",
    "        \"table_id\": entry[\"table_id\"],\n",
    "        \"sub_answer\": subanswer\n",
    "    }\n",
    "\n",
    "    answer_fw.write(json.dumps(tmp) + \"\\n\")\n",
    "\n",
    "answer_fw.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6492c988-ee25-426d-b9a5-2a100f79009a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Read the table and text regarding \"Stockholm Marathon\" to answer the following question.\n",
      "\n",
      "The table contains important information and this is the introduction of the table:\n",
      "The Stockholm Marathon, known as the ASICS Stockholm Marathon for sponsorship reasons, is an annual marathon arranged in Stockholm, Sweden, since 1979. It serves as the Swedish marathon championship race. At the 2009 Stockholm Marathon more than 18,500 participants (14,442 men and 4,385 women) were registered. [citation needed]\n",
      "\n",
      "Year | Athlete | Country | Time ( h : m : s )\n",
      "1979 | Jukka Toivola | Finland | 2:17:35\n",
      "1980 | Jeff Wells | United States | 2:15:49\n",
      "1981 | Bill Rodgers | United States | 2:13:26\n",
      "1982 | Kjell-Erik Ståhl | Sweden - Hässleholms AIS | 2:19:20\n",
      "1983 | Hugh Jones | United Kingdom | 2:11:37\n",
      "1984 | Agapius Masong | Tanzania | 2:13:47\n",
      "1985 | Tommy Persson | Sweden - Heleneholms IF | 2:17:18\n",
      "1986 | Kjell-Erik Ståhl | Sweden - Enhörna IF | 2:12:33\n",
      "1987 | Kevin Forster | United Kingdom | 2:13:52\n",
      "1988 | Suleiman Nyambui | Tanzania | 2:14:26\n",
      "1989 | Dave Clarke | United Kingdom | 2:13:34\n",
      "1990 | Simon Naali | Tanzania | 2:13:04\n",
      "1991 | Åke Eriksson | Sweden - Hässelby SK | 2:12:38\n",
      "1992 | Hugh Jones | United Kingdom | 2:15:58\n",
      "1993 | Daniel Mbuli | South Africa | 2:16:30\n",
      "1994 | Tesfaye Bekele | Ethiopia | 2:14:06\n",
      "1995 | Åke Eriksson | Sweden - Hässelby SK | 2:14:29\n",
      "1996 | Tesfaye Bekele | Ethiopia | 2:15:05\n",
      "1997 | Benson Masya | Kenya | 2:17:22\n",
      "1998 | Martin Ojuko | Kenya | 2:16:12\n",
      "\n",
      "I believe the following text information will help answer the question:\n",
      "William Henry Bill Rodgers nicknamed Boston Billy ( born December 23 , 1947 ) is an American runner , Olympian and former American record holder in the marathon . Rodgers is best known for his four victories in both the Boston Marathon , including three straight 1978-1980 and the New York City Marathon between 1976 and 1980 .\n",
      "\n",
      "Please think step by step, you should answer only in a word, a number, or a date, instead of a whole sentence. Hint: Using same words from the text and table as answer can achieve better correct rate.\n",
      "\n",
      "Question: How many times did the person who finished the Stockholm Marathon with a time of 2:13:26 win the Boston Marathon ?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(response_raw[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f21225-f092-49f4-8b2b-a087131f0991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
