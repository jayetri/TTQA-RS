{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f65186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "028281c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--option\", default=\"none\", type=str)\n",
    "parser.add_argument(\"--model\", default=\"llama3\", type=str, help=\"qwen1.5-14b-chat and qwen-turbo are better\")\n",
    "parser.add_argument(\"--start\", default=0, type=int)\n",
    "parser.add_argument(\"--end\", default=None, type=int)\n",
    "parser.add_argument(\n",
    "    \"--temperature\",\n",
    "    type=float,\n",
    "    default=0.7,\n",
    "    help=\"temperature of 0 implies greedy sampling.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--traced_json_file\",\n",
    "    default=r\"retrieved_data\",\n",
    "    type=str,\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--text_path\",\n",
    "    default=r\"WikiTables-WithLinks-master\\request_tok\",\n",
    "    \n",
    "    type=str,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--table_path\",\n",
    "    default=r\"WikiTables-WithLinks-master\\tables_tok\",\n",
    "    \n",
    "    type=str,\n",
    ")\n",
    "\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e279553",
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstration = {}\n",
    "demonstration[\"none\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dca55ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(args):\n",
    "    data_test_traced = json.load(open(args.traced_json_file, \"r\"))\n",
    "    data_list = []\n",
    "    for sample in tqdm(data_test_traced[args.start:args.end]):\n",
    "        table_id = sample[\"table_id\"]\n",
    "        question_data = None\n",
    "        for q_data in questions_data:\n",
    "            if q_data['table_id'] == table_id:\n",
    "                question_data = q_data\n",
    "                break\n",
    "        if question_data is None:\n",
    "            print(f\"No question data found for {table_id}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            tables_tok_path = f\"WikiTables-WithLinks-master\\\\tables_tok\\\\{table_id}.json\"\n",
    "            with open(tables_tok_path, 'r') as f:\n",
    "                table_data = json.load(f)\n",
    "        except Exception:\n",
    "            print(f\"The file {table_id} does not exist.\")\n",
    "            continue\n",
    "\n",
    "        # Read text data\n",
    "        try:\n",
    "            text_file = os.path.join(args.text_path, f\"{table_id}.json\")\n",
    "            with open(text_file, \"r\") as f:\n",
    "                text_data = json.load(f)\n",
    "        except Exception:\n",
    "            print(f\"The file {text_file} does not exist.\")\n",
    "            continue\n",
    "            \n",
    "        question_text = sample[\"question\"]\n",
    "        answer_text = sample[\"pred\"]\n",
    "         # Extract wiki links from nodes and target\n",
    "        wikis = [\n",
    "            node[2]\n",
    "            for node in sample[\"nodes\"]\n",
    "            if node[2] is not None and node[2].startswith(\"/wiki\")\n",
    "        ]\n",
    "        \n",
    "        target_wiki = sample[\"target\"][2]\n",
    "        if target_wiki and target_wiki.startswith(\"/wiki\"):\n",
    "            wikis.append(target_wiki)\n",
    "        \n",
    "        # Get the corresponding text for each wiki link\n",
    "        wiki_text = \"\"\n",
    "        if wikis:\n",
    "            wiki_lines = [text_data.get(wiki, \"\") for wiki in wikis]\n",
    "            wiki_text = \"\\n\".join(wiki_lines)\n",
    "        \n",
    "        \n",
    "        # Create a DataFrame from the table data\n",
    "        df = pd.DataFrame(\n",
    "            [tuple(zip(*row))[0] for row in table_data[\"data\"]],\n",
    "            columns=list(zip(*table_data[\"header\"]))[0],\n",
    "        )\n",
    "\n",
    "        data_list.append({\n",
    "            \"table_id\": table_id,\n",
    "            \"question\": question_text,\n",
    "            \"answer\": answer_text,\n",
    "            \"table\": df,\n",
    "            \"wiki\": wiki_text,\n",
    "            \"title\": table_data[\"title\"],\n",
    "            \"intro\": table_data[\"intro\"]\n",
    "        })\n",
    "\n",
    "    return data_list\n",
    "\n",
    "questions_path = \"Data/HybridQA/test.json\"\n",
    "with open(questions_path, 'r') as f:\n",
    "    questions_data = json.load(f)\n",
    "\n",
    "def df_format(data):\n",
    "    try:\n",
    "        formatted_str = \" | \".join(data.columns) + \"\\n\"\n",
    "        for _, row in data.iterrows():\n",
    "            row_str = \" | \".join([str(row[col]) for col in data.columns])\n",
    "            formatted_str += row_str + \"\\n\"\n",
    "        return formatted_str\n",
    "    except Exception as e:\n",
    "        #print(f\"Error formatting table: {data}, error: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56254a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3820"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d_%H_%M\")\n",
    "fw = open(f\"outputs/response_s{args.start}_e{args.end}_{args.option}_{args.model}_{dt_string}.json\", \"w\",)\n",
    "tmp = {\"demonstration\": demonstration[args.option]}\n",
    "fw.write(json.dumps(tmp) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a55fc01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████████████████▏                                                      | 189/600 [00:09<00:20, 20.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file List_of_songs_in_The_Beatles:_Rock_Band_0 does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|█████████████████████████████████████▋                                          | 283/600 [00:14<00:14, 21.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file ISO_3166-2:KN_1 does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|███████████████████████████████████████████                                     | 323/600 [00:17<00:13, 21.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file Roy_\"Royalty\"_Hamilton_0 does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|████████████████████████████████████████████████████████████▌                   | 454/600 [00:25<00:05, 25.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file List_of_Summer_Olympics_venues:_L_0 does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|████████████████████████████████████████████████████████████████████████████▏   | 571/600 [00:31<00:01, 23.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file Magic:_The_Gathering_Hall_of_Fame_0 does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|██████████████████████████████████████████████████████████████████████████████▉ | 592/600 [00:32<00:00, 23.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file ISO_3166-2:FR_2 does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 600/600 [00:32<00:00, 18.26it/s]\n"
     ]
    }
   ],
   "source": [
    "data_list = read_data(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e532ff16",
   "metadata": {},
   "source": [
    "load llama model here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6dc51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for entry in tqdm(data_list):\n",
    "    question = entry['question']\n",
    "    answer = entry['answer']\n",
    "    \n",
    "    # Generate multiple reasoning chains\n",
    "    reasoning_chains = []\n",
    "    for _ in range(3):  # Generate 3 reasoning chains\n",
    "        prompt = demonstration[args.option] + '\\n\\n'\n",
    "        prompt += f'Read the table and text regarding \"{entry[\"title\"]}\" to answer the question.\\n\\n'\n",
    "        prompt += df_format(entry['table']) + '\\n'\n",
    "        \n",
    "        if entry['wiki']:\n",
    "            prompt += \"Text\" + '\\n' + entry['wiki'] + '\\n\\n'\n",
    "        prompt += 'Question: ' + question + '\\nAnswer:'\n",
    "        \n",
    "        response_raw = query({'inputs': prompt})\n",
    "        try:\n",
    "            response = response_raw[0].get('generated_text', '').split('\\nAnswer:')[5].split('Reasoning process')[0].strip()\n",
    "        except KeyError:\n",
    "            response = ''\n",
    "        response = response.split('\\n')[0].strip()\n",
    "        reasoning_chains.append(response)\n",
    "    \n",
    "    # Combine reasoning chains for self-consistency\n",
    "    combined_prompt = f\"Based on the following reasoning chains, provide the most reasonable answer:\\n\\n\"\n",
    "    for i, chain in enumerate(reasoning_chains, 1):\n",
    "        combined_prompt += f\"Chain {i}: {chain}\\n\\n\"\n",
    "    combined_prompt += f\"Question: {question}\\nFinal Answer:\"\n",
    "    \n",
    "    final_response_raw = query({'inputs': combined_prompt})\n",
    "    try:\n",
    "        final_response = final_response_raw[0].get('generated_text', '').split('Final Answer:')[1].strip()\n",
    "    except IndexError:\n",
    "        final_response = ''\n",
    "    \n",
    "    tmp = {\n",
    "        \"question\": question,\n",
    "        \"response\": final_response,\n",
    "        \"answer\": answer,\n",
    "        \"table_id\": entry[\"table_id\"],\n",
    "        \"reasoning_chains\": reasoning_chains\n",
    "    }\n",
    "    fw.write(json.dumps(tmp) + \"\\n\")\n",
    "\n",
    "fw.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
